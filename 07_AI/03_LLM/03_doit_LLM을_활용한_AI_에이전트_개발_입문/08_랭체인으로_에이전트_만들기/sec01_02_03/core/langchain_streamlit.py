import streamlit as st
from config.config import config
from core.openai_model import get_model
from tools.generate_chat_id import generate_chat_id
from langchain_openai import ChatOpenAI
from langchain_core.chat_history import InMemoryChatMessageHistory, BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage

def get_session_history(session_id:str) -> BaseChatMessageHistory:
    if session_id not in st.session_state["store"]:
        st.session_state["store"][session_id] = InMemoryChatMessageHistory()
    return st.session_state["store"][session_id]

def streamlit_chat(stream=config.getboolean("setting", "mode.stream")):
    
    # ì‚¬ì´ë“œë°” : 
    with st.sidebar:
        "Streamlit í…ŒìŠ¤íŠ¸"
    
    # title
    st.title("ğŸ¤– Doit Chatbot")
    
    # ì´ˆê¸° ë©”ì‹œì§€
    if "messages" not in st.session_state: # st.session_state : ìŠ¤íŠ¸ë¦¼ë¦¿ì—ì„œ ì‚¬ìš©ìì˜ ì„¸ì…˜ ìƒíƒœ ê´€ë¦¬
        st.session_state.messages = [SystemMessage("ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê²Œ ë‹µí•˜ëŠ” AI ì±—ë´‡ì…ë‹ˆë‹¤.")]
        st.session_state.messages.append(AIMessage("ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"))
    
    # ëŒ€í™” ì„¸ì…˜ ìŠ¤í† ì–´ ìƒì„±
    if "store" not in st.session_state:
        st.session_state["store"] = {}
    
    llm = get_model()
    with_message_history = RunnableWithMessageHistory(llm, get_session_history)
    
    # ì„¸ì…˜ ìŠ¤í† ì–´ì—ì„œ í˜„ì¬ ì„¸ì…˜idë¥¼ ì°¾ê³ , ì—†ìœ¼ë©´ ìƒì„±
    session_id = ""
    if "session_id" not in st.session_state:
        session_id = generate_chat_id()
        st.session_state["session_id"] = session_id
    model_config = {"configurable":{"session_id":session_id}}
    
    # ëŒ€í™” ê¸°ë¡ì„ ì›¹ë¸Œë¼ìš°ì €ì— ì¶œë ¥
    for mgs in st.session_state.messages:
        if mgs:
            if (isinstance(mgs, SystemMessage)) and (config.getboolean("setting", "mode.debug")):
                st.chat_message("system").write(mgs.content)
            if isinstance(mgs, AIMessage):
                st.chat_message("assistant").write(mgs.content)
            if isinstance(mgs, HumanMessage):
                st.chat_message("user").write(mgs.content)
    
    # LLM ì— ì§ˆì˜
    if prompt := st.chat_input(): # ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë°›ì•„ prompt ë³€ìˆ˜ì— í• ë‹¹
        user_message = HumanMessage(prompt)
        st.session_state.messages.append(user_message)
        st.chat_message("user").write(prompt)
        
        # ì§ˆì˜ì— ëŒ€í•œ ì‘ë‹µ ë°›ì•„ì˜´
        response = None
        content = None
        if stream:
            response_chunk = with_message_history.stream(user_message, config=model_config)
            response = None
            with st.chat_message("assistant").empty():
                for r in response_chunk:
                    if response is None:
                        response = r
                    else:
                        response += r
                    st.markdown(response.content)
            content = response.content
        else:
            response = with_message_history.invoke(user_message, config=model_config)
            content = response.content
            st.chat_message("assistant").write(content)
        st.session_state.messages.append(response)
        st.session_state.messages.append({"role":"assistant", "content":content})
    
# streamlit ì‹¤í–‰
# uv run streamlit run main.py
