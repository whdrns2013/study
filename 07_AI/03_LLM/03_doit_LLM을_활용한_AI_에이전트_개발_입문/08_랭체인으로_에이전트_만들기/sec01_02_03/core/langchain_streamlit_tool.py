import streamlit as st
from config.config import config
from core.openai_model import get_model
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage, ToolMessage
from tools.langchain_tools import tools, tool_dict

def streamlit_chat(stream=config.getboolean("setting", "mode.stream")):
    
    # ì‚¬ì´ë“œë°” : 
    with st.sidebar:
        "Streamlit í…ŒìŠ¤íŠ¸"
    
    # title
    st.title("ğŸ¤– Doit Chatbot")
    
    # ëª¨ë¸ ì„ ì–¸
    llm = get_model()
    llm_with_tools = llm.bind_tools(tools)
    
    # ì´ˆê¸° ë©”ì‹œì§€
    if "messages" not in st.session_state: # st.session_state : ìŠ¤íŠ¸ë¦¼ë¦¿ì—ì„œ ì‚¬ìš©ìì˜ ì„¸ì…˜ ìƒíƒœ ê´€ë¦¬
        st.session_state.messages = [SystemMessage("ë‹¹ì‹ ì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì¹œì ˆí•˜ê²Œ ë‹µí•˜ëŠ” AI ì±—ë´‡ì…ë‹ˆë‹¤.")]
        st.session_state.messages.append(AIMessage("ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"))
    
    # ëŒ€í™” ê¸°ë¡ì„ ì›¹ë¸Œë¼ìš°ì €ì— ì¶œë ¥
    for msg in st.session_state.messages:
        if msg:
            if (isinstance(msg, SystemMessage)) and (config.getboolean("setting", "mode.debug")):
                st.chat_message("system").write(msg.content)
                st.chat_message("tool").write(msg.content)
            if isinstance(msg, AIMessage):
                st.chat_message("assistant").write(msg.content)
            if isinstance(msg, HumanMessage):
                st.chat_message("user").write(msg.content)
    
    # LLM ì— ì§ˆì˜
    if prompt := st.chat_input(): # ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë°›ì•„ prompt ë³€ìˆ˜ì— í• ë‹¹
        user_message = HumanMessage(prompt)
        st.session_state.messages.append(user_message)
        st.chat_message("user").write(prompt)
        
        # ì§ˆì˜ì— ëŒ€í•œ ì‘ë‹µ ë°›ì•„ì˜´
        if stream:
            # ì‚¬ìš©ì ì§ˆë¬¸ì— ëŒ€í•œ ì‘ë‹µ
            response = llm_with_tools.stream(st.session_state.messages)

            # garhering
            gathered = None

            def stream_gen():
                nonlocal gathered
                for chunk in response:
                    # chunk ëˆ„ì 
                    if gathered is None:
                        gathered = chunk
                    else:
                        gathered += chunk
                    yield chunk.content or ""

            # âš ï¸ ì¶œë ¥í•˜ì§€ ì•Šê³  ìˆ˜ì§‘ë§Œ
            _ = list(stream_gen())

            # gathered == AIMessage
            if (gathered is not None):
                st.session_state.messages.append(gathered)
            
            # tool ì²˜ë¦¬
            if gathered and gathered.tool_calls:
                for tool_call in gathered.tool_calls:
                    tool = tool_dict[tool_call["name"]]
                    tool_msg = tool.invoke(tool_call)
                    st.session_state.messages.append(tool_msg)
                
                # tool ê²°ê³¼ ë°˜ì˜ ì‘ë‹µ
                final_response = llm_with_tools.stream(st.session_state.messages)
                final_gathered = None

                def final_stream_gen():
                    nonlocal final_gathered
                    for chunk in final_response:
                        if final_gathered is None:
                            final_gathered = chunk
                        else:
                            final_gathered += chunk
                        yield chunk.content or ""

                result = st.chat_message("assistant").write_stream(final_stream_gen())
                st.session_state.messages.append(final_gathered)

            else:
                if gathered:
                    st.chat_message("assistant").write(gathered.content)
            
        else:
            response = llm_with_tools.invoke(st.session_state.messages)
            result = response.content
            st.chat_message("assistant").write(result) # write : ì¶œë ¥ë§Œ í•¨ (ë°˜í™˜ ì—†ìŒ)
            st.session_state.messages.append(response)
        
    
# streamlit ì‹¤í–‰
# uv run streamlit run main.py
