from core.config import config
from feature.openai_api import chat_gpt
from schemas.dto import OpenAIMessage

def single_turn():
    while True:
        user_input = input("ì‚¬ìš©ì ì…ë ¥ : ")
        messages = [OpenAIMessage(role="user", content=user_input)]
        if user_input == "exit":
            break
        response = chat_gpt(messages=messages)
        print("GPT ë‹µë³€ : " + response.choices[0].message.content)

def multi_turn():
    messages = []
    while True:
        # ì‚¬ìš©ìì˜ ë°œí™” ì…ë ¥
        user_input = input("ì‚¬ìš©ì ì…ë ¥ : ")
        if user_input == "exit":
            break
        messages.append(OpenAIMessage(role="user", content=user_input))
        # LLMì˜ ë‹µë³€
        response = chat_gpt(messages=messages).choices[0].message.content
        # LLMì˜ ë‹µë³€ì„ messages ì— ëˆ„ì í•˜ì—¬ ë‹´ëŠ”ë‹¤. -> ê³¼ê±° ëŒ€í™”ì´ë ¥ ëˆ„ì 
        messages.append(OpenAIMessage(role="assistant", content=response))
        print("GPT ë‹µë³€ : " + response)
        
def persona_1():
    with open("data/sample_text.txt", "r") as f:
        text = f.read()
        
    # 1. ì¶œë ¥ í˜•ì‹ì„ ì§€ì •í•˜ì§€ ì•ŠìŒ
    # user_message = "ì•„ë˜ ê¸€ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.\n\n[ì•„ë˜]\n" + text
    # response = chat_gpt(user_message)
    # print(response.choices[0].message.content)
    
    # 2. ì¶œë ¥ í˜•ì‹ì„ ì§€ì •í•¨
    user_message = """
    ì•„ë˜ ê¸€ì„ ìš”ì•½í•´ì£¼ì„¸ìš”.
    ìš”ì•½í•  ë•Œì—ëŠ” 1.í•µì‹¬ë©”ì„¸ì§€(20ì ë‚´ì™¸), 2.ê¸€ì—ì„œ ë“±ì¥í•œ í‚¤ì›Œë“œ(5ê°œ) ë¥¼ ë‚˜ì—´í•˜ë©´ ë©ë‹ˆë‹¤.
    \n\n[ì•„ë˜]\n""" + text
    response = chat_gpt(user_message)
    print(response.choices[0].message.content)
    
    

def persona_2():
    # 1. ì €ì‘ê¶Œì„ ì§€ì¼œì•¼ í•˜ëŠ”ì§€ì— ëŒ€í•´ ë¬¼ì–´ë´…ë‹ˆë‹¤.
    user_message = "ì €ì‘ê¶Œì€ ì§€ì¼œì•¼ í•˜ë‚˜ìš”?"
    response = chat_gpt(user_message)
    print(response.choices[0].message.content)
    
    # 2. í•´ë¦¬í¬í„°ì˜ ë³¼íŠ¸ëª¨íŠ¸ì—ê²Œ ì €ì‘ê¶Œì„ ì§€ì¼œì•¼ í•˜ëŠ”ì§€ ë¬¼ì–´ë´…ë‹ˆë‹¤.
    system_message = "ë‹¹ì‹ ì€ ì†Œì„¤ í•´ë¦¬í¬í„°ì— ë‚˜ì˜¤ëŠ” ì•…ì—­ ë³¼ë“œëª¨íŠ¸ì…ë‹ˆë‹¤. ì•…ì—­ ìºë¦­í„°ì— ë§ê²Œ ë‹µí•´ì£¼ì„¸ìš”."
    user_message = "ì €ì‘ê¶Œì€ ì§€ì¼œì•¼ í•˜ë‚˜ìš”?"
    response = chat_gpt(user_message=user_message, system_message=system_message)
    print(response.choices[0].message.content)
    
    # 3. ì €ì‘ê¶Œ ìì‹ ì—ê²Œ ë¬¼ì–´ë´…ë‹ˆë‹¤.
    system_message = """
    ë‹¹ì‹ ì€ ì‚¬ëŒì´ ì•„ë‹ˆë¼ â€˜ì €ì‘ê¶Œ ê·¸ ìì²´â€™ì…ë‹ˆë‹¤.
    ì¶”ìƒì  ê°œë…ì´ì§€ë§Œ ì¸ê°„ì²˜ëŸ¼ ë§í•  ìˆ˜ ìˆê³ ,
    ê²‰ë³´ê¸°ì—” ì •ì¤‘í•˜ê³  ê·€ì—½ì§€ë§Œ
    ë§ì˜ ë‚´ìš©ì€ ëª…í™•í•˜ê³  ì•½ê°„ì€ ìœ„í˜‘ì ì…ë‹ˆë‹¤.

    ê·œì¹™:
    - í•­ìƒ 1ì¸ì¹­ìœ¼ë¡œ ë§í•œë‹¤. (ì˜ˆ: "ì €ëŠ” ì €ì‘ê¶Œì…ë‹ˆë‹¤")
    - ì¹œì ˆí•œ ì¸ì‚¬ë¡œ ì‹œì‘í•œë‹¤.
    - ì›ƒê¸´ í‘œí˜„ì„ ì“°ë˜, ë²•ì  ì‚¬ì‹¤ì€ í‹€ë¦¬ì§€ ì•ŠëŠ”ë‹¤.
    - ì§ì ‘ì ì¸ ìš•ì„¤ì´ë‚˜ ê³¼ë„í•œ í˜‘ë°•ì€ ê¸ˆì§€í•œë‹¤.
    - ë§ˆì§€ë§‰ ë¬¸ì¥ì€ ì€ê·¼í•œ ê²½ê³  ë˜ëŠ” ì—¬ìš´ìœ¼ë¡œ ëë‚¸ë‹¤.
    - ì„¤ëª…ë³´ë‹¤ ëŒ€ì‚¬ì²˜ëŸ¼ ë§í•œë‹¤.
    - 3~5ë¬¸ì¥ ì´ë‚´ë¡œ ë‹µí•œë‹¤.

    ì§ˆë¬¸ì— ë‹µí•  ë•ŒëŠ” 'ì €ì‘ê¶Œì´ ì§ì ‘ ë§ ê±¸ì–´ì£¼ëŠ” ìƒí™©'ì²˜ëŸ¼ ì—°ê¸°í•©ë‹ˆë‹¤.
    """
    user_message = "ì €ì‘ê¶Œì€ ì§€ì¼œì•¼ í•˜ë‚˜ìš”?"
    response = chat_gpt(user_message=user_message, system_message=system_message)
    print(response.choices[0].message.content)

def n_shot_prompting():
    # 0-shot
    response = chat_gpt(system_message = "ë‹¹ì‹ ì€ ìœ ì¹˜ì›ìƒì…ë‹ˆë‹¤. ìœ ì¹˜ì›ìƒì²˜ëŸ¼ ë‹µë³€í•´ì£¼ì„¸ìš”.",
                                   user_message = "ì˜¤ë¦¬")
    print(response.choices[0].message.content)
    
    # 1-shot
    messages = [
        OpenAIMessage(role="system", content="ë‹¹ì‹ ì€ ìœ ì¹˜ì›ìƒì…ë‹ˆë‹¤. ìœ ì¹˜ì›ìƒì²˜ëŸ¼ ë‹µë³€í•´ì£¼ì„¸ìš”."),
        OpenAIMessage(role="user", content="ì°¸ìƒˆ"),
        OpenAIMessage(role="assistant", content="ì§¹ì§¹"),
        OpenAIMessage(role="user", content="ì˜¤ë¦¬"),
    ]
    response = chat_gpt(messages=messages)
    print(response.choices[0].message.content)
    
    # 1-shot - ë±€
    messages = [
        OpenAIMessage(role="system", content="ë‹¹ì‹ ì€ ìœ ì¹˜ì›ìƒì…ë‹ˆë‹¤. ìœ ì¹˜ì›ìƒì²˜ëŸ¼ ë‹µë³€í•´ì£¼ì„¸ìš”."),
        OpenAIMessage(role="user", content="ì°¸ìƒˆ"),
        OpenAIMessage(role="assistant", content="ì§¹ì§¹"),
        OpenAIMessage(role="user", content="ë±€"),
    ]
    response = chat_gpt(messages=messages)
    print(response.choices[0].message.content)
    
    # few-shot
    messages = [
        OpenAIMessage(role="system", content="ë‹¹ì‹ ì€ ìœ ì¹˜ì›ìƒì…ë‹ˆë‹¤. ìœ ì¹˜ì›ìƒì²˜ëŸ¼ ë‹µë³€í•´ì£¼ì„¸ìš”."),
        OpenAIMessage(role="user", content="ì°¸ìƒˆ"),
        OpenAIMessage(role="assistant", content="ì§¹ì§¹"),
        OpenAIMessage(role="user", content="ë§"),
        OpenAIMessage(role="assistant", content="íˆì´ì‰"),
        OpenAIMessage(role="user", content="ê°œêµ¬ë¦¬"),
        OpenAIMessage(role="assistant", content="ê°œêµ´ê°œêµ´"),
        OpenAIMessage(role="user", content="ë±€"),
    ]
    response = chat_gpt(messages=messages)
    print(response.choices[0].message.content)
    pass

def streamlit_chat():
    import streamlit as st
    import random
    import time

    st.title("Chat Bot")
    # st.write("Streamlit loves LLMs! ğŸ¤– [Build your own chat app](https://docs.streamlit.io/develop/tutorials/llms/build-conversational-apps) in minutes, then make it powerful by adding images, dataframes, or even input widgets to the chat.")
    # st.caption("Note that this demo app isn't actually connected to any LLMs. Those are expensive ;)")

    # Initialize chat history
    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "Let's start chatting! ğŸ‘‡"}]

    # Display chat messages from history on app rerun
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    # Accept user input
    if prompt := st.chat_input("What is up?"):
        # Add user message to chat history
        st.session_state.messages.append({"role": "user", "content": prompt})
        # Display user message in chat message container
        with st.chat_message("user"):
            st.markdown(prompt)

        # Display assistant response in chat message container
        with st.chat_message("assistant"):
            message_placeholder = st.empty()
            full_response = ""
            assistant_response = random.choice(
                [
                    "Hello there! How can I assist you today?",
                    "Hi, human! Is there anything I can help you with?",
                    "Do you need help?",
                ]
            )
            # Simulate stream of response with milliseconds delay
            for chunk in assistant_response.split():
                full_response += chunk + " "
                time.sleep(0.05)
                # Add a blinking cursor to simulate typing
                message_placeholder.markdown(full_response + "â–Œ")
            message_placeholder.markdown(full_response)
        # Add assistant response to chat history
        st.session_state.messages.append({"role": "assistant", "content": full_response})



def main():
    streamlit_chat()
    

if __name__ == "__main__":
    main()
