
## 스트림 출력하기  

### 스트림 출력  

![](https://whdrns2013.github.io/assets/images/20250717_001_001.gif)  

- 답변이 타이핑하듯이 순서대로 출력되는 방식  
- 지금까지 살펴본 GPT 응답의 출력은, 모든 답변 텍스트가 준비된 후 한 번에 반환되는 방식  
- 그에비해 위 이미지를 보면 한글자씩 스트림 출력  

### yield  

- 스트림 출력을 알아보기 전에 yield  
- https://whdrns2013.github.io/python/20260129_002_python_yield/  
- yield는 함수를 generator로 만드는 키워드  
- 즉, 함수를 모두 실행한 뒤 한 번에 결과를 반환하는 게 아니라, 결과가 생성될 때마다 반환하는 방식  
- 오래 걸리는 작업을 순차적으로 할 때 유용하다.  

### 스트림 출력을 적용하기  

#### stream 옵션  

- openai 기준으로, 응답을 요청할 때 stream이라는 옵션을 True 로 해주면 된다.  

```python
response = client.chat.completions.create(
        model = model,
        temperature = temperature,
        messages = messages,
        tools = tools,
        stream = True # stream
    )
```

- 이 상태로 터미널로 출력해보면 아래와 같이 에러가 나온다.  

```python
user    :지금은 몇시야?
Traceback (most recent call last):
  ...
    if response.choices[0].message.tool_calls:
       ^^^^^^^^^^^^^^^^
AttributeError: 'Stream' object has no attribute 'choices'
```

- 이는 stream을 적용했을 때 답변의 구조가 달라서 그렇다.  
- 아래는 stream을 적용하지 않았을 때와, 적용했을 때 답변의 구조 차이이다.  

#### 일반 답변과 stream 답변의 구조 차이 (openai)  


<div style="display: flex; gap: 20px;">
<div style="flex: 1;" markdown="1">

```python
# Stream 적용 전 (전체답변)
ChatCompletion(
    id='chatcmpl-...',
    choices=[
        Choice(
            finish_reason='stop',
            index=0,
            logprobs=None,
            message=ChatCompletionMessage(
                content='안녕하세요! 무엇을 도와드릴까요?',
                refusal=None,
                role='assistant',
                annotations=[],
                audio=None,
                function_call=None,
                tool_calls=None
            )
        )
    ],
    created=1769659391,
    model='gpt-4o-2024-08-06',
    object='chat.completion',
    service_tier='default',
    system_fingerprint='fp_...',
    usage=CompletionUsage(
        completion_tokens=12,
        prompt_tokens=268,
        total_tokens=280,
        ...
    )
)
```

<div style="flex: 1;" markdown="1">

```python
{
    'response': <Response [200 OK]>,
    '_cast_to': <class 'openai.types.chat.chat_completion_chunk.ChatCompletionChunk'>,
    '_client': <openai.OpenAI object at 0x000001618841B8F0>,
    '_decoder': <openai._streaming.SSEDecoder object at 0x000001618A0718E0>,
    '_iterator': <generator object Stream.__stream__ at 0x0000016189EA2200>,
    '__orig_class__': openai.Stream[openai.types.chat.chat_completion_chunk.ChatCompletionChunk]
}
```

generator로, 이를 반복해서 확인해보면  

```python
ChatCompletionChunk(
    id='chatcmpl-...',
    choices=[
        Choice(
            delta=ChoiceDelta(
                content='',
                function_call=None,
                refusal=None,
                role='assistant',
                tool_calls=None
            ),
            finish_reason=None,
            index=0,
            logprobs=None)
        ],
        created=1769661151,
        model='gpt-4o-2024-08-06',
        object='chat.completion.chunk',
        service_tier='default',
        system_fingerprint='fp_...',
        usage=None,
        obfuscation='eN...')

ChatCompletionChunk( .. choices=[ .. (delta=ChoiceDelta(content='안',  ...  
ChatCompletionChunk( .. choices=[ .. (delta=ChoiceDelta(content='녕하세요', ...  
ChatCompletionChunk( .. choices=[ .. (delta=ChoiceDelta(content='!', ...  
ChatCompletionChunk( .. choices=[ .. (delta=ChoiceDelta(content=' 어떻게', ...
ChatCompletionChunk( .. choices=[ .. (delta=ChoiceDelta(content=' 도', ...  
ChatCompletionChunk( .. choices=[ .. (delta=ChoiceDelta(content='와', ...  
ChatCompletionChunk( .. choices=[ .. (delta=ChoiceDelta(content='드', ...  
ChatCompletionChunk( .. choices=[ .. (delta=ChoiceDelta(content='릴', ...  
ChatCompletionChunk( .. choices=[ .. (delta=ChoiceDelta(content='까요', ...  
ChatCompletionChunk( .. choices=[ .. (delta=ChoiceDelta(content='?', ...  
ChatCompletionChunk( .. choices=[ .. (delta=ChoiceDelta(content=None, ... finish_reason='stop', index=0, logprobs=None)
```

</div>
</div>

- 구조 차이 요약  

|항목|일반답변|stream답변|
|---|---|---|
|응답횟수|1회에 전부|여러 차례에 나눠서 순차적으로|
|응답객체|ChatCompletion|ChatCompletionChunk 여러 개|
|답변 텍스트|choices[].message.content<br>완성된 답변 전체|choices[].delta<br>조각난 필드 단위로|
|role|한 번에 role:assistant 포함|첫 chunk 에만 존재(choice.delta.role)<br>나머지 chunk 에는 role:None이 담김|
|응답 중지 정보||마지막 chunk에 finish_reason|
|tool/func 호출|한 번에 완성된 형태|arguments가 여러 chunk로 쪼개어 옴|


