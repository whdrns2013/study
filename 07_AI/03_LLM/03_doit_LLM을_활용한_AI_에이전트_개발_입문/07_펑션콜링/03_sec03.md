
## ìŠ¤íŠ¸ë¦¼ ì¶œë ¥í•˜ê¸°  

### ìŠ¤íŠ¸ë¦¼ ì¶œë ¥  

![](https://whdrns2013.github.io/assets/images/20250717_001_001.gif)  

- ë‹µë³€ì´ íƒ€ì´í•‘í•˜ë“¯ì´ ìˆœì„œëŒ€ë¡œ ì¶œë ¥ë˜ëŠ” ë°©ì‹  
- ì§€ê¸ˆê¹Œì§€ ì‚´í´ë³¸ GPT ì‘ë‹µì˜ ì¶œë ¥ì€, ëª¨ë“  ë‹µë³€ í…ìŠ¤íŠ¸ê°€ ì¤€ë¹„ëœ í›„ í•œ ë²ˆì— ë°˜í™˜ë˜ëŠ” ë°©ì‹  
- ê·¸ì—ë¹„í•´ ìœ„ ì´ë¯¸ì§€ë¥¼ ë³´ë©´ í•œê¸€ìì”© ìŠ¤íŠ¸ë¦¼ ì¶œë ¥  


### stream ì˜µì…˜  

- openai ê¸°ì¤€ìœ¼ë¡œ, ì‘ë‹µì„ ìš”ì²­í•  ë•Œ streamì´ë¼ëŠ” ì˜µì…˜ì„ True ë¡œ í•´ì£¼ë©´ ëœë‹¤.  

```python
response = client.chat.completions.create(
        model = model,
        temperature = temperature,
        messages = messages,
        tools = tools,
        stream = True # stream
    )
```

- ì´ ìƒíƒœë¡œ í„°ë¯¸ë„ë¡œ ì¶œë ¥í•´ë³´ë©´ ì•„ë˜ì™€ ê°™ì´ ì—ëŸ¬ê°€ ë‚˜ì˜¨ë‹¤.  

```python
user    :ì§€ê¸ˆì€ ëª‡ì‹œì•¼?
Traceback (most recent call last):
  ...
    if response.choices[0].message.tool_calls:
       ^^^^^^^^^^^^^^^^
AttributeError: 'Stream' object has no attribute 'choices'
```

- ì´ëŠ” streamì„ ì ìš©í–ˆì„ ë•Œ ë‹µë³€ì˜ êµ¬ì¡°ê°€ ë‹¬ë¼ì„œ ê·¸ë ‡ë‹¤.  
- ì•„ë˜ëŠ” streamì„ ì ìš©í•˜ì§€ ì•Šì•˜ì„ ë•Œì™€, ì ìš©í–ˆì„ ë•Œ ë‹µë³€ì˜ êµ¬ì¡° ì°¨ì´ì´ë‹¤.  

### ì¼ë°˜ ë‹µë³€ê³¼ stream ë‹µë³€ì˜ êµ¬ì¡° ì°¨ì´ (openai)  


<div style="display: flex; gap: 20px;">
<div style="flex: 1;" markdown="1">

```python
# Stream ì ìš© ì „ (ì „ì²´ë‹µë³€)
ChatCompletion(
    id='chatcmpl-...',
    choices=[
        Choice(
            finish_reason='stop',
            index=0,
            logprobs=None,
            message=ChatCompletionMessage(
                content='ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?',
                refusal=None,
                role='assistant',
                annotations=[],
                audio=None,
                function_call=None,
                tool_calls=None
            )
        )
    ],
    created=1769659391,
    model='gpt-4o-2024-08-06',
    object='chat.completion',
    service_tier='default',
    system_fingerprint='fp_...',
    usage=CompletionUsage(
        completion_tokens=12,
        prompt_tokens=268,
        total_tokens=280,
        ...
    )
)
```

<div style="flex: 1;" markdown="1">

```python
{
    'response': <Response [200 OK]>,
    '_cast_to': <class 'openai.types.chat.chat_completion_chunk.ChatCompletionChunk'>,
    '_client': <openai.OpenAI object at 0x000001618841B8F0>,
    '_decoder': <openai._streaming.SSEDecoder object at 0x000001618A0718E0>,
    '_iterator': <generator object Stream.__stream__ at 0x0000016189EA2200>,
    '__orig_class__': openai.Stream[openai.types.chat.chat_completion_chunk.ChatCompletionChunk]
}
```

generatorë¡œ, ì´ë¥¼ ë°˜ë³µí•´ì„œ í™•ì¸í•´ë³´ë©´  

```python
ChatCompletionChunk(
    id='chatcmpl-...',
    choices=[
        Choice(
            delta=ChoiceDelta(
                content='',
                function_call=None,
                refusal=None,
                role='assistant',
                tool_calls=None
            ),
            finish_reason=None,
            index=0,
            logprobs=None)
        ],
        created=1769661151,
        model='gpt-4o-2024-08-06',
        object='chat.completion.chunk',
        service_tier='default',
        system_fingerprint='fp_...',
        usage=None,
        obfuscation='eN...')

ChatCompletionChunk( .. choices=[ .. (delta=ChoiceDelta(content='ì•ˆ',  ...  
ChatCompletionChunk( .. choices=[ .. (delta=ChoiceDelta(content='ë…•í•˜ì„¸ìš”', ...  
ChatCompletionChunk( .. choices=[ .. (delta=ChoiceDelta(content='!', ...  
ChatCompletionChunk( .. choices=[ .. (delta=ChoiceDelta(content=' ì–´ë–»ê²Œ', ...
ChatCompletionChunk( .. choices=[ .. (delta=ChoiceDelta(content=' ë„', ...  
ChatCompletionChunk( .. choices=[ .. (delta=ChoiceDelta(content='ì™€', ...  
ChatCompletionChunk( .. choices=[ .. (delta=ChoiceDelta(content='ë“œ', ...  
ChatCompletionChunk( .. choices=[ .. (delta=ChoiceDelta(content='ë¦´', ...  
ChatCompletionChunk( .. choices=[ .. (delta=ChoiceDelta(content='ê¹Œìš”', ...  
ChatCompletionChunk( .. choices=[ .. (delta=ChoiceDelta(content='?', ...  
ChatCompletionChunk( .. choices=[ .. (delta=ChoiceDelta(content=None, ... finish_reason='stop', index=0, logprobs=None)
```

</div>
</div>

- êµ¬ì¡° ì°¨ì´ ìš”ì•½  

|í•­ëª©|ì¼ë°˜ë‹µë³€|streamë‹µë³€|
|---|---|---|
|ì‘ë‹µíšŸìˆ˜|1íšŒì— ì „ë¶€|ì—¬ëŸ¬ ì°¨ë¡€ì— ë‚˜ëˆ ì„œ ìˆœì°¨ì ìœ¼ë¡œ|
|ì‘ë‹µê°ì²´|ChatCompletion|ChatCompletionChunk ì—¬ëŸ¬ ê°œ|
|ë‹µë³€ í…ìŠ¤íŠ¸|choices[].message.content<br>ì™„ì„±ëœ ë‹µë³€ ì „ì²´|choices[].delta<br>ì¡°ê°ë‚œ í•„ë“œ ë‹¨ìœ„ë¡œ|
|role|í•œ ë²ˆì— role:assistant í¬í•¨|ì²« chunk ì—ë§Œ ì¡´ì¬(choice.delta.role)<br>ë‚˜ë¨¸ì§€ chunk ì—ëŠ” role:Noneì´ ë‹´ê¹€|
|ì‘ë‹µ ì¤‘ì§€ ì •ë³´||ë§ˆì§€ë§‰ chunkì— finish_reason|
|tool/func í˜¸ì¶œ|í•œ ë²ˆì— ì™„ì„±ëœ í˜•íƒœ|argumentsê°€ ì—¬ëŸ¬ chunkë¡œ ìª¼ê°œì–´ ì˜´|

### ìŠ¤íŠ¸ë¦¼ ì¶œë ¥ì„ ì ìš©í•˜ê¸°   

#### yield  

- ìŠ¤íŠ¸ë¦¼ ì¶œë ¥ì„ ì•Œì•„ë³´ê¸° ì „ì— yield  
- https://whdrns2013.github.io/python/20260129_002_python_yield/  
- yieldëŠ” í•¨ìˆ˜ë¥¼ generatorë¡œ ë§Œë“œëŠ” í‚¤ì›Œë“œ  
- ì¦‰, í•¨ìˆ˜ë¥¼ ëª¨ë‘ ì‹¤í–‰í•œ ë’¤ í•œ ë²ˆì— ê²°ê³¼ë¥¼ ë°˜í™˜í•˜ëŠ” ê²Œ ì•„ë‹ˆë¼, ê²°ê³¼ê°€ ìƒì„±ë  ë•Œë§ˆë‹¤ ë°˜í™˜í•˜ëŠ” ë°©ì‹  
- ì˜¤ë˜ ê±¸ë¦¬ëŠ” ì‘ì—…ì„ ìˆœì°¨ì ìœ¼ë¡œ í•  ë•Œ ìœ ìš©í•˜ë‹¤.  


#### ì½”ë“œ  

- ì‚¬ì‹¤ yield ë¥¼ ì“¸ í•„ìš”ê°€ ì—†ì—ˆë‹¤.  
- ì´ë¯¸ response ê°€ ì‹¤ì‹œê°„ ì œë„ˆë ˆì´í„°ì´ê¸° ë•Œë¬¸  

```python
from openai import OpenAI
from config.config import config
from schemas.dto import OpenAIMessage
import json
from tools.gpt_functions import tools, tool_mapping

def get_ai_response(messages:list[dict],
                    model:str,
                    temperature:float,
                    tools=None,
                    stream=False):
    api_key = config["apikey"]["openai"]
    client = OpenAI(api_key = api_key)
    response = client.chat.completions.create(
        model = model,
        temperature = temperature,
        messages = messages,
        tools = tools,
        stream = stream
    )
    
    return response # response ê°€ ì´ë¯¸ ì‹¤ì‹œê°„ ì œë„ˆë ˆë¦¬í„°ë¼ ë³„ë„ yield ë¥¼ ì‚¬ìš©í•  í•„ìš”ê°€ ì—†ìŒ  

def chat_gpt(user_message:str|None=None,
             system_message:str|None=None,
             assistant_message:str|None=None,
             messages:list[OpenAIMessage]|list[dict]|None=None,
             model:str="gpt-4o",
             temperature:float=0.1,
             tools=tools,
             stream=False):
    
    # ë©”ì‹œì§€ ì œì‘
    if messages:
        for i, message in enumerate(messages):
            if isinstance(message, OpenAIMessage):
                messages[i] = message.__dict__
    else:
        messages = []
        if system_message:
            messages.append({"role":"system", "content":system_message})
        if user_message:
            messages.append({"role":"user", "content":user_message})
        if assistant_message:
            messages.append({"role":"assistant", "content":assistant_message})
    
    # AI ì‘ë‹µ ë°˜í™˜
    response = get_ai_response(messages=messages,
                               model=model,
                               temperature=temperature,
                               tools=tools,
                               stream=stream)
    
    # function calling ì²˜ë¦¬
    if hasattr(response, "choices"):
        if response.choices[0].message.tool_calls:
            tool_calls = response.choices[0].message.tool_calls
            for tool_call in tool_calls:
                # LLM ì‘ë‹µì—ì„œ ë„êµ¬ ì •ë³´ ì¶”ì¶œ
                tool_name = tool_call.function.name
                tool_call_id = tool_call.id
                arguments = json.loads(tool_call.function.arguments) 
                
                # ë„êµ¬ë§ˆë‹¤ ì‚¬ìš© í•¨ìˆ˜ ë§¤ì¹­í•´ì„œ ë„êµ¬ì˜ ê²°ê³¼ë¥¼ ë©”ì‹œì§€ë¡œ ìƒì„±
                messages = tool_mapping(messages = messages,
                                        tool_name = tool_name,
                                        tool_call_id = tool_call_id,
                                        arguments = arguments)
            
            # í•œë²ˆ ë” gpt ì‘ë‹µ í˜¸ì¶œ, ì´ë²ˆì—” ë„êµ¬ì˜ ê²°ê³¼ì™€ ë„êµ¬ í˜¸ì¶œ idë¥¼ í¬í•¨í•¨
            messages.append({"role":"system", "content":"ì´ì œ ì£¼ì–´ì§„ ê²°ê³¼ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì‹œì˜¤"})
            response = get_ai_response(messages, model=model, temperature=temperature, tools=tools)
        
    return response

# ê²°ê³¼ í…ìŠ¤íŠ¸ ì¶œë ¥
# response.choices[0].message.content

# TODO: ìŠ¤íŠ¸ë¦¬ë° ì ìš©
```

- ìŠ¤íŠ¸ë¦¼ë¦¿ì— streamì„ ì ìš©í•˜ê¸° ìœ„í•´ì„œëŠ” ìŠ¤íŠ¸ë¦¼ë¦¿ì˜ íŠ¹ì • ê·œì¹™ì— ë”°ë¼ ì„¤ì •í•´ì•¼ í•œë‹¤.  

```python
# LLM ì— ì§ˆì˜
if prompt := st.chat_input():
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)
    # ì§ˆì˜ì— ëŒ€í•œ ì‘ë‹µ ë°›ì•„ì˜´
    response = chat_function(messages = st.session_state.messages, stream = stream)
    content = ""
    if stream: # -------------------------------------------------- (1) stream ì¸ ê²½ìš°
        with st.chat_message("assistant").empty(): # -------------- (2) streamlit ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë°©ì‹
            for chunk in response:
                content_chunk = chunk.choices[0].delta.content
                if content_chunk:
                    content += content_chunk
                    st.markdown(content) # ------------------------ (3) ì§€ê¸ˆê¹Œì§€ì˜ ë‹µë³€ì„ ëª¨ì•„ ë§ˆí¬ë‹¤ìš´ ì¶œë ¥
    else:
        # í™”ë©´ì— ì‘ë‹µ ì¶œë ¥
        with st.chat_message("assistant"):
            content = response.choices[0].message.content
            st.markdown(content)
    # ì‘ë‹µì„ ì„¸ì…˜ì— ì—…ë°ì´íŠ¸
    st.session_state.messages.append({"role":"assistant", "content":content})
    
```

- ì´ìŠˆê°€ í•˜ë‚˜ ìˆì—ˆëŠ”ë°, stream ì„ì—ë„ í…ìŠ¤íŠ¸ê°€ í•œ ê¸€ìì”©ì´ ì•„ë‹ˆë¼ í•œ ë¬¸ë‹¨ì”© ë‚˜ì˜¤ëŠ” ë¬¸ì œ.  
- ì´ëŠ” ì¶œë ¥ ë²„í¼ ë•Œë¬¸ì´ë¼ëŠ”ë° ìì„¸í•œ ë‚´ìš©ì€ ì•„ë˜ì— ì •ë¦¬í•´ë’€ë‹¤.  

[https://whdrns2013.github.io/python/20260129_003_python_print_flush/](https://whdrns2013.github.io/python/20260129_003_python_print_flush/)  


### ìŠ¤íŠ¸ë¦¼ ë°©ì‹ì—ì„œ í‘ì…˜ ì½œë§ ì ìš©í•˜ê¸°  

stream ì´ Trueì¸ ìƒíƒœì—ì„œ í‘ì…˜ ì½œë§ì„ ì‚¬ìš©í•˜ë©´ ê·¸ ê²°ê³¼ë„ ì¡°ê° ë‹¨ìœ„ë¡œ ë°˜í™˜ëœë‹¤.  
ì´ë¥¼ í„°ë¯¸ë„ ì°½ì—ì„œ ì¶œë ¥í•˜ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.  

```bash
user : ì„œìš¸ê³¼ ë‰´ìš•ì˜ ì‹œê°„ì€?
[
 ChoiceDeltaToolCall(index=0, id='...', function=ChoiceDeltaToolCallFunction(arguments='', name='get_current_time'), type='function'),
 ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='{"ti', name=None), type=None),
 ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='mezon', name=None), type=None),
 ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='e": "A', name=None), type=None),
 ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='sia/', name=None), type=None),
 ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='Seoul', name=None), type=None),
 ChoiceDeltaToolCall(index=0, id=None, function=ChoiceDeltaToolCallFunction(arguments='"}', name=None), type=None),
 ChoiceDeltaToolCall(index=1, id='...', function=ChoiceDeltaToolCallFunction(arguments='', name='get_current_time'), type='function'),
 ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='{"ti', name=None), type=None),
 ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='mezon', name=None), type=None),
 ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='e": "A', name=None), type=None),
 ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='meri', name=None), type=None),
 ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='ca/Ne', name=None), type=None),
 ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='w_York', name=None), type=None),
 ChoiceDeltaToolCall(index=1, id=None, function=ChoiceDeltaToolCallFunction(arguments='"}', name=None), type=None)
]
```

ì²« ë©”ì‹œì§€ì—ì„œ tool_calls ë¡œ get_current_time í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•¨ì„ ì•Œë¦¬ê³ 
ê·¸ ë‹¤ìŒ ë©”ì‹œì§€ë¶€í„° argument ê°€ ì˜¤ê³  ìˆìœ¼ë©°  
ë§ˆì§€ë§‰ì— finish reason ì´ ì˜¤ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤.  

ì´ì œ ì´ê²ƒì„ ì¡°í•©í•´ì„œ ì²˜ë¦¬ë  ìˆ˜ ìˆë„ë¡ í•´ë³´ì  

ë¨¼ì €, ìœ„ì— ë³´ì´ëŠ” í‘ì…˜ ì½œì„ í•˜ë‚˜ë¡œ í•©ì³ë³´ì  

```python
from dataclasses import dataclass, asdict
from collections import defaultdict

@dataclass
class OpenAIFunction:
    argument    :str|None=""
    name        :str|None=None

@dataclass
class ToolObject:
    id          :str|None=None
    function    :OpenAIFunction|None=None
    type        :str|None=None

def tool_list_to_tool_obj(tools:list):
    tool_map = defaultdict(lambda: ToolObject(function=OpenAIFunction()))
    
    for tool_call in tools:
        to = tool_map[tool_call.index]
        
        # tool call ID
        if tool_call.id is not None:
            to.id = tool_call.id
        
        # function name
        if tool_call.function.name is not None:
            to.function.name = tool_call.function.name
    
        # arguments
        to.function.argument += tool_call.function.arguments
        
        # tool call type
        if tool_call.type is not None:
            to.type = tool_call.type
    
    return {"tool_calls" : [asdict(to) for to in tool_map.values()]}
```

ì´ë¥¼ ì¶œë ¥í•´ë³´ë©´  

```bash
[
    {'id': 'call_7De2tY63dCiFyyXHDd360P5K',
     'function': {'argument': '{"timezone": "Asia/Seoul"}', 'name': 'get_current_time'},
     'type': 'function'
    },
    {'id': 'call_1eDALRelCYUc1d970zEQ8bLs',
     'function': {'argument': '{"timezone": "America/New_York"}','name': 'get_current_time'},
     'type': 'function'
    }
]
```

ì´ë¥¼ í„°ë¯¸ë„ ì¶œë ¥ê³¼ ì—°ê²°í•´ë³´ë©´  

```python
from core.openai_api import chat_gpt
from tools.gpt_functions import tools
from config.config import config

def terminal_chat(stream=False):
    messages = []
    while True:
        inp = input("user\t:")
        if inp == "exit":
            break
        messages.append({"role":"user", "content":inp})
        response = chat_gpt(messages=messages, tools=tools, stream=stream)
        if stream:
            content = ""
            for chunk in response:
                if hasattr(chunk, "choices"):
                    content_chunk = chunk.choices[0].delta.content
                    if content_chunk:
                        print(content_chunk, end="", flush=True) # print ê°€ ì¶œë ¥ ë²„í¼ë¥¼ ì±„ìš°ëŠë¼ ë­‰ì³ì„œ ë‚˜ì˜¤ëŠ” ê²½ìš°ê°€ ìˆìŒ -> ê°•ì œë¡œ ë°”ë¡œ ì¶œë ¥í•˜ë„ë¡ í•¨
                        content += content_chunk
                    else:
                        print("\n")
            ai_message = content
        else:
            ai_message = response.choices[0].message.content
            print(ai_message)
        messages.append({"role":"assistant", "content":ai_message})
```

- ì§ˆì˜ì‘ë‹µ

```bash
user    :ì„œìš¸ì˜ ì‹œê°„ì€?
í˜„ì¬ ì„œìš¸ì˜ ì‹œê°„ì€ 2026ë…„ 1ì›” 30ì¼ ì˜¤ì „ 2ì‹œ 47ë¶„ì…ë‹ˆë‹¤.

user    :ì„œìš¸ê³¼ ë‰´ìš•ì˜ ì‹œê°„ì€?
í˜„ì¬ ì„œìš¸ì˜ ì‹œê°„ì€ 2026ë…„ 1ì›” 30ì¼ ì˜¤ì „ 2ì‹œ 48ë¶„ì´ê³ , ë‰´ìš•ì˜ ì‹œê°„ì€ 2026ë…„ 1ì›” 29ì¼ ì˜¤í›„ 12ì‹œ 48ë¶„ì…ë‹ˆë‹¤.
```

### ìµœì¢… ìŠ¤íŠ¸ë¦¼ë¦¿ì— ì ìš©í•˜ê¸°  

- ì±…ê³¼ ì¡°ê¸ˆ ë‹¤ë¥´ê²Œ ì§„í–‰í–ˆê³ , ì•„ì˜ˆ gptë‹¨ê³¼ tool ë‹¨ì—ì„œ stream ë¶„ê¸°ì²˜ë¦¬ë¥¼ ëª¨ë‘ ì§„í–‰í–ˆê¸°ì— streamlitì—ì„œ ë³„ë„ ì²˜ë¦¬í•  ê±´ ì—†ë‹¤  

```python
import streamlit as st
from core.openai_api import chat_gpt
from config.config import config

def streamlit_chat(chat_function=chat_gpt, stream=False):
    
    # ì‚¬ì´ë“œë°” : 
    with st.sidebar:
        "Streamlit í…ŒìŠ¤íŠ¸"
    
    # title
    st.title("ğŸ¤– Doit Chatbot")
    
    # ì´ˆê¸° ë©”ì‹œì§€
    if "messages" not in st.session_state: # st.session_state : ìŠ¤íŠ¸ë¦¼ë¦¿ì—ì„œ ì‚¬ìš©ìì˜ ì„¸ì…˜ ìƒíƒœ ê´€ë¦¬
                                           # ì‚¬ìš©ìê°€ ì›¹ ë¸Œë¼ìš°ì €ì—ì„œ ìƒí˜¸ì‘ìš©í•˜ëŠ” ë™ì•ˆ ìƒíƒœê°€ ì €ì¥/ìœ ì§€ë˜ê³  ì—…ë°ì´íŠ¸ ë¨
        st.session_state.messages = [{"role": "assistant", "content": "ì•ˆë…•í•˜ì„¸ìš”! ë¬´ì—‡ì„ ë„ì™€ë“œë¦´ê¹Œìš”?"}] # ì„¸ì…˜ì´ ì—†ìœ¼ë©´ ì´ˆê¸° ë©”ì‹œì§€ ë³´ì—¬ì¤Œ
    
    # ëŒ€í™” ê¸°ë¡ì„ ì›¹ë¸Œë¼ìš°ì €ì— ì¶œë ¥
    for message in st.session_state.messages:
        if config["setting"]["mode.debug"] == "1": # debug ëª¨ë“œì¸ ê²½ìš° ëª¨ë“  ë©”ì‹œì§€ ì¶œë ¥
            expose_message = True
        else: # debug ëª¨ë“œê°€ ì•„ë‹Œ ê²½ìš° ìœ ì € ì…ë ¥ê³¼ LLM ë‹µë³€ë§Œ ì¶œë ¥
            expose_message = (message["role"] == "user") or (message["role"] == "assistant")
        if expose_message:
            with st.chat_message(message["role"]): # st.chat_message : ìŠ¤íŠ¸ë¦¼ë¦¿ì˜ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤ì— ë©”ì‹œì§€ë¥¼ ì¶œë ¥í•˜ëŠ” ìš©ë„ (assistant / user)
                st.markdown(message["content"])    # st.markdown : ì»¨í…ì¸ ë¥¼ ì¶œë ¥í•  í˜•íƒœ ì§€ì • - ë§ˆí¬ë‹¤ìš´ìœ¼ë¡œ (ê¸°ë³¸ì€ write)
    
    # LLM ì— ì§ˆì˜
    if prompt := st.chat_input(): # ì‚¬ìš©ìì˜ ì…ë ¥ì„ ë°›ì•„ prompt ë³€ìˆ˜ì— í• ë‹¹
        st.session_state.messages.append({"role": "user", "content": prompt}) # ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ ìƒíƒœ - ë©”ì„¸ì§€ì— ì¶”ê°€
        with st.chat_message("user"): # ì‚¬ìš©ìì˜ ì§ˆë¬¸ì„ í™”ë©´ì˜ "chat message container"ë¡œ ë³´ì—¬ì¤Œ
            st.markdown(prompt)
        # ì§ˆì˜ì— ëŒ€í•œ ì‘ë‹µ ë°›ì•„ì˜´
        response = chat_function(messages = st.session_state.messages, stream = stream)
        content = ""
        if stream:
            with st.chat_message("assistant").empty():
                for chunk in response:
                    content_chunk = chunk.choices[0].delta.content
                    if content_chunk:
                        content += content_chunk
                        st.markdown(content)
        else:
            # í™”ë©´ì— ì‘ë‹µ ì¶œë ¥
            with st.chat_message("assistant"):
                content = response.choices[0].message.content
                st.markdown(content)
        # ì‘ë‹µì„ ì„¸ì…˜ì— ì—…ë°ì´íŠ¸
        st.session_state.messages.append({"role":"assistant", "content":content})
    
# streamlit ì‹¤í–‰
# uv run streamlit run main.py
```

### streamitì— tools ë‚´ìš© ì¶œë ¥í•˜ê¸°  

- streamlit ì— í‘ì…˜ì½œë§ëœ ë‚´ìš©ì„ ìš”ì•½í•´ì„œ(tool_calls dict í˜•íƒœ) ë³´ì—¬ì£¼ë ¤ê³  í•œë‹¤.  
- ì´ë¥¼ ìœ„í•´ chat-gpt í•¨ìˆ˜ê°€ tool_calls ë¥¼ ë°˜í™˜í•˜ë„ë¡ ìˆ˜ì •í–ˆê³   

```python
from openai import OpenAI
from config.config import config
from schemas.dto import OpenAIMessage
import json
from tools.gpt_functions import tools, tool_mapping
from utils.tool_list_to_tool_obj import tool_list_to_tool_obj

def get_ai_response(messages:list[dict],
                    model:str,
                    temperature:float,
                    tools=None,
                    stream=False):
    api_key = config["apikey"]["openai"]
    client = OpenAI(api_key = api_key)
    response = client.chat.completions.create(
        model = model,
        temperature = temperature,
        messages = messages,
        tools = tools,
        stream = stream
    )
    return response # response ê°€ ì´ë¯¸ ì‹¤ì‹œê°„ ì œë„ˆë ˆë¦¬í„°ë¼ ë³„ë„ yield ë¥¼ ì‚¬ìš©í•  í•„ìš”ê°€ ì—†ìŒ  

def chat_gpt(user_message:str|None=None,
             system_message:str|None=None,
             assistant_message:str|None=None,
             messages:list[OpenAIMessage]|list[dict]|None=None,
             model:str="gpt-4o",
             temperature:float=0.1,
             tools=tools,
             stream=False):
    
    # ë©”ì‹œì§€ ì œì‘
    if messages:
        for i, message in enumerate(messages):
            if isinstance(message, OpenAIMessage):
                messages[i] = message.__dict__
    else:
        messages = []
        if system_message:
            messages.append({"role":"system", "content":system_message})
        if user_message:
            messages.append({"role":"user", "content":user_message})
        if assistant_message:
            messages.append({"role":"assistant", "content":assistant_message})
    
    # AI ì‘ë‹µ ë°˜í™˜
    response = get_ai_response(messages=messages,
                               model=model,
                               temperature=temperature,
                               tools=tools,
                               stream=stream)
    
    # function calling ì²˜ë¦¬
    tool_calls = []
    if stream:
        tool_calls_chunk = []
        for chunk in response:
            if hasattr(chunk, "choices"):
                if chunk.choices[0].delta.tool_calls:
                    tool_calls_chunk += chunk.choices[0].delta.tool_calls
        tool_obj = tool_list_to_tool_obj(tool_calls_chunk)
        tool_calls = tool_obj["tool_calls"]
        for tool_call in tool_calls:
            tool_name = tool_call["function"]["name"]
            tool_call_id = tool_call["id"]
            arguments = json.loads(tool_call["function"]["arguments"])
            messages = tool_mapping(messages = messages,
                                        tool_name = tool_name,
                                        tool_call_id = tool_call_id,
                                        arguments = arguments)
        messages.append({"role":"system", "content":"ì´ì œ ì£¼ì–´ì§„ ê²°ê³¼ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì‹œì˜¤"})
        response = get_ai_response(messages, model=model, temperature=temperature, tools=tools, stream=stream)
    else:
        if hasattr(response, "choices"):
            if response.choices[0].message.tool_calls:
                tool_calls = response.choices[0].message.tool_calls
                for tool_call in tool_calls:
                    # LLM ì‘ë‹µì—ì„œ ë„êµ¬ ì •ë³´ ì¶”ì¶œ
                    tool_name = tool_call.function.name
                    tool_call_id = tool_call.id
                    arguments = json.loads(tool_call.function.arguments) 
                    
                    # ë„êµ¬ë§ˆë‹¤ ì‚¬ìš© í•¨ìˆ˜ ë§¤ì¹­í•´ì„œ ë„êµ¬ì˜ ê²°ê³¼ë¥¼ ë©”ì‹œì§€ë¡œ ìƒì„±
                    messages = tool_mapping(messages = messages,
                                            tool_name = tool_name,
                                            tool_call_id = tool_call_id,
                                            arguments = arguments)
                
                # í•œë²ˆ ë” gpt ì‘ë‹µ í˜¸ì¶œ, ì´ë²ˆì—” ë„êµ¬ì˜ ê²°ê³¼ì™€ ë„êµ¬ í˜¸ì¶œ idë¥¼ í¬í•¨í•¨
                messages.append({"role":"system", "content":"ì´ì œ ì£¼ì–´ì§„ ê²°ê³¼ë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€í•˜ì‹œì˜¤"})
                response = get_ai_response(messages, model=model, temperature=temperature, tools=tools, stream=stream)
        
    return response, tool_calls
```

- streamlit ì—ì„œëŠ” tool_callsê°€ 1ê°œ ì´ìƒì¸ ê²½ìš° í™”ë©´ì— íˆ´ ë‚´ìš©ì„ ì¶œë ¥í•˜ë„ë¡ í–ˆë‹¤.  
- st.expander ëŠ” í¼ì¹˜ê¸°/ì ‘ê¸° ë¥¼ ì œê³µí•˜ë©°, expanded=False ë¥¼ ì¸ìê°’ìœ¼ë¡œ ì£¼ë©´ ê¸°ë³¸ì ìœ¼ë¡œ ì ‘íŒ ìƒíƒœë¡œ ë‚˜ì˜¨ë‹¤.  

```python
# tool_calls ë¥¼ í™”ë©´ì— ì¶œë ¥
if len(tool_calls) > 0:
    tool_call_msg = [tool_call["function"] for tool_call in tool_calls]
    with st.expander("tool calls", expanded=False):
        st.write(tool_call_msg)
```

[alt text](/assets/images/sec03_final.png)