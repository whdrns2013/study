{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7450e125",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8d8b4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_corpus = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6b1c0a1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "class WhitespaceTokenizer():\n",
    "    def tokenize(self, input:str) -> list[str]:\n",
    "        if isinstance(input, str):\n",
    "            result = input.split(\" \")\n",
    "        return result\n",
    "\n",
    "# Text Cleaner\n",
    "class TextCleaner:\n",
    "    def __init__(self):\n",
    "        # Create a set of frequent words\n",
    "        self.stopwords = set('for a of the and to in'.split(' '))\n",
    "    def clean_text(self, words:list[str]) -> list[str]:\n",
    "        # Lowercase each document, split it by white space and filter out stopwords\n",
    "        words = [word.lower() for word in words if word.lower() not in self.stopwords]\n",
    "        return words\n",
    "\n",
    "# filter by frequency\n",
    "class FilterByFrequency:\n",
    "    def __init__(self):\n",
    "        # Count word frequencies\n",
    "        from collections import defaultdict\n",
    "        self.frequency_dict = defaultdict(int)\n",
    "    def make_filter(self, docs:list[list[str]]):\n",
    "        for text in docs:\n",
    "            for token in text:\n",
    "                self.frequency_dict[token] += 1\n",
    "    def filter(self, words:list[str], threshold:int=1):\n",
    "        # Only keep words that appear more than once\n",
    "        filtered_words = [token for token in words if self.frequency_dict[token] > threshold]\n",
    "        return filtered_words\n",
    "\n",
    "# bow\n",
    "class BagOfWords:\n",
    "    def __init__(self):\n",
    "        self.dictionary:dict[str,int]|None=None\n",
    "    def create_dictionary(self, input:list[list[str]]):\n",
    "        from gensim import corpora\n",
    "        self.dictionary = corpora.Dictionary(input)\n",
    "    def represent_bow(self, input:list[list[str]]):\n",
    "        bow_corpus = [self.dictionary.doc2bow(text) for text in input]\n",
    "        return bow_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8b65531d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== corpus(words) =====\n",
      "[['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response', 'time'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time'], ['trees'], ['graph', 'trees'], ['graph', 'minors', 'trees'], ['graph', 'minors', 'survey']]\n",
      "===== BoW =====\n",
      "[[(0, 1), (1, 1), (2, 1)], [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)], [(2, 1), (5, 1), (7, 1), (8, 1)], [(1, 1), (5, 2), (8, 1)], [(3, 1), (6, 1), (7, 1)], [(9, 1)], [(9, 1), (10, 1)], [(9, 1), (10, 1), (11, 1)], [(4, 1), (10, 1), (11, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# (1) 토크나이징 : 공백을 기준으로\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "tokenized_docs = [tokenizer.tokenize(doc) for doc in text_corpus]\n",
    "# (2) 텍스트 클리닝 - lower + stopwords\n",
    "text_cleaner = TextCleaner()\n",
    "cleaned_docs = [text_cleaner.clean_text(words) for words in tokenized_docs]\n",
    "# (3) 빈도 기반 필터링 : 1회 발생 단어는 제외\n",
    "filter = FilterByFrequency()\n",
    "filter.make_filter(cleaned_docs)\n",
    "processed_corpus = [filter.filter(doc, 1) for doc in cleaned_docs]\n",
    "# (4) BoW 생성\n",
    "bow_model = BagOfWords()\n",
    "bow_model.create_dictionary(processed_corpus)\n",
    "bow = bow_model.represent_bow(processed_corpus)\n",
    "\n",
    "print(\"===== corpus(words) =====\")\n",
    "print(processed_corpus)\n",
    "print(\"===== BoW =====\")\n",
    "print(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87c22723",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== BoW =====\n",
      "[(0, 1), (1, 1)]\n",
      "===== BoW Token Dictionary =====\n",
      "{'computer': 0, 'human': 1, 'interface': 2, 'response': 3, 'survey': 4, 'system': 5, 'time': 6, 'user': 7, 'eps': 8, 'trees': 9, 'graph': 10, 'minors': 11}\n"
     ]
    }
   ],
   "source": [
    "# 새로운 문장 BoW\n",
    "new_sentence = \"Human computer interaction\"\n",
    "cleaned_words = filter.filter(text_cleaner.clean_text(tokenizer.tokenize(new_sentence)))\n",
    "new_vec = bow_model.dictionary.doc2bow(cleaned_words)\n",
    "print(\"===== BoW =====\")\n",
    "print(new_vec)\n",
    "print(\"===== BoW Token Dictionary =====\")\n",
    "print(bow_model.dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9bde3f61",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['human', 'interface', 'computer'],\n",
       " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
       " ['eps', 'user', 'interface', 'system'],\n",
       " ['system', 'human', 'system', 'eps'],\n",
       " ['user', 'response', 'time'],\n",
       " ['trees'],\n",
       " ['graph', 'trees'],\n",
       " ['graph', 'minors', 'trees'],\n",
       " ['graph', 'minors', 'survey']]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acef411",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bow-bag-of-words",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
