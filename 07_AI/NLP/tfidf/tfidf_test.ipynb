{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64bf3acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63f77e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus\n",
    "\n",
    "text_corpus = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef7b83b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "class WhitespaceTokenizer():\n",
    "    def tokenize(self, input:str) -> list[str]:\n",
    "        if isinstance(input, str):\n",
    "            result = input.split(\" \")\n",
    "        return result\n",
    "\n",
    "# Text Cleaner\n",
    "class TextCleaner:\n",
    "    def __init__(self):\n",
    "        # Create a set of frequent words\n",
    "        self.stopwords = set('for a of the and to in'.split(' '))\n",
    "    def clean_text(self, words:list[str]) -> list[str]:\n",
    "        # Lowercase each document, split it by white space and filter out stopwords\n",
    "        words = [word.lower() for word in words if word.lower() not in self.stopwords]\n",
    "        return words\n",
    "\n",
    "# filter by frequency\n",
    "class FilterByFrequency:\n",
    "    def __init__(self):\n",
    "        # Count word frequencies\n",
    "        from collections import defaultdict\n",
    "        self.frequency_dict = defaultdict(int)\n",
    "    def make_filter(self, docs:list[list[str]]):\n",
    "        for text in docs:\n",
    "            for token in text:\n",
    "                self.frequency_dict[token] += 1\n",
    "    def filter(self, words:list[str], threshold:int=1):\n",
    "        # Only keep words that appear more than once\n",
    "        filtered_words = [token for token in words if self.frequency_dict[token] > threshold]\n",
    "        return filtered_words\n",
    "\n",
    "# bow\n",
    "class BagOfWords:\n",
    "    def __init__(self):\n",
    "        self.dictionary:dict[str,int]|None=None\n",
    "    def create_dictionary(self, input:list[list[str]]):\n",
    "        from gensim import corpora\n",
    "        self.dictionary = corpora.Dictionary(input)\n",
    "    def represent_bow(self, input:list[list[str]]):\n",
    "        bow_corpus = [self.dictionary.doc2bow(text) for text in input]\n",
    "        return bow_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e692d011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) 토크나이징 : 공백을 기준으로\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "tokenized_docs = [tokenizer.tokenize(doc) for doc in text_corpus]\n",
    "# (2) 텍스트 클리닝 - lower + stopwords\n",
    "text_cleaner = TextCleaner()\n",
    "cleaned_docs = [text_cleaner.clean_text(words) for words in tokenized_docs]\n",
    "# (3) 빈도 기반 필터링 : 1회 발생 단어는 제외\n",
    "filter = FilterByFrequency()\n",
    "filter.make_filter(cleaned_docs)\n",
    "processed_corpus = [filter.filter(doc, 1) for doc in cleaned_docs]\n",
    "# (4) BoW 생성\n",
    "bow_model = BagOfWords()\n",
    "bow_model.create_dictionary(processed_corpus)\n",
    "bow = bow_model.represent_bow(processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c16ac09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF (전통적 TF-IDF)\n",
    "from collections import Counter, defaultdict\n",
    "from math import log\n",
    "\n",
    "class StandardTFIDF:\n",
    "    def __init__(self, corpus:list[list[str]]=None):\n",
    "        self.corpus:list[list[str]] = corpus\n",
    "        self.vocabulary:dict[int,str]\n",
    "        self.total_doc_count:int\n",
    "        self.document_frequency:dict[str,int]\n",
    "        if corpus:\n",
    "            self.fit(corpus)\n",
    "    \n",
    "    def _make_vocabulary(self, corpus:list[list[str]]) -> dict[str,int]:\n",
    "        unique_tokens = sorted(set([token for doc in corpus for token in doc]))\n",
    "        vocabulary = dict()\n",
    "        for idx, token in enumerate(unique_tokens):\n",
    "            vocabulary[token] = idx\n",
    "        self.vocabulary = vocabulary\n",
    "        return vocabulary\n",
    "    \n",
    "    def _total_doc_count(self, corpus:list[list[str]]) -> int:\n",
    "        return len(corpus)\n",
    "    \n",
    "    def _document_frequency(self, corpus:list[list[str]]) -> dict[str,int]:\n",
    "        df = defaultdict(int)\n",
    "        for doc in corpus:\n",
    "            for token in set(doc):\n",
    "                df[token] += 1\n",
    "        return df\n",
    "    \n",
    "    def id2token(self) -> dict[int,str]:\n",
    "        id2token = {idx:token for token, idx in self.vocabulary.items()}\n",
    "        return id2token\n",
    "    \n",
    "    def token2id(self) -> dict[str,int]:\n",
    "        token2id = self.vocabulary\n",
    "        return token2id\n",
    "    \n",
    "    def _convert_to_vector(self, tfidf:dict[str,float|int]):\n",
    "        vector = [0] * len(self.vocabulary)\n",
    "        for token, score in tfidf.items():\n",
    "            idx = self.vocabulary[token]\n",
    "            vector[idx] = score\n",
    "        return vector\n",
    "    \n",
    "    def _tf(self, document:list[str]) -> dict[str,int|float]:\n",
    "        \n",
    "        cnt = Counter(document)\n",
    "        tf = {token : (cnt[token]/len(document)) for token in cnt}\n",
    "        return tf\n",
    "    \n",
    "    def _idf(self, document:list[str]) -> dict[str,int|float]:\n",
    "        idf = dict()\n",
    "        document_count = self.total_doc_count\n",
    "        cnt = Counter(document)\n",
    "        for token in cnt:\n",
    "            idf[token] = log(document_count / self.document_frequency[token])\n",
    "        return idf\n",
    "    \n",
    "    def _tfidf(self, document:list[str]) -> dict[str,int|float]:\n",
    "        tfidf = dict()\n",
    "        tf = self._tf(document)\n",
    "        idf = self._idf(document)\n",
    "        cnt = Counter(document)\n",
    "        for token in cnt:\n",
    "            tfidf[token] = tf[token] * idf[token]\n",
    "        return tfidf\n",
    "    \n",
    "    def fit(self, corpus:list[list[str]]) -> None:\n",
    "        self.corpus = corpus\n",
    "        self.vocabulary = self._make_vocabulary(corpus)\n",
    "        self.total_doc_count = self._total_doc_count(corpus)\n",
    "        self.document_frequency = self._document_frequency(corpus)\n",
    "    \n",
    "    def transform(self, documents:list[list[str]] | list[str]):\n",
    "        result:list[list[float|int]] = list()\n",
    "        if isinstance(documents, list) and documents and isinstance(documents[0], str):\n",
    "            documents = [documents]\n",
    "        if len(documents) == 0:\n",
    "            return []\n",
    "        for doc in documents:\n",
    "            if len(doc) == 0:\n",
    "                result.append([0] * len(self.vocabulary))\n",
    "            else:\n",
    "                tfidf = self._tfidf(doc)\n",
    "                result.append(self._convert_to_vector(tfidf))\n",
    "        return result\n",
    "    \n",
    "    def fit_transform(self, corpus:list[list[str]], documents:list[list[str]] | list[str]=None):\n",
    "        if documents is None:\n",
    "            documents = corpus.copy()\n",
    "        self.fit(corpus)\n",
    "        result = self.transform(documents)\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01da202b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['system', 'minors']\n",
      "tf : {'system': 0.5, 'minors': 0.5}\n",
      "idf : {'system': 1.0986122886681098, 'minors': 1.5040773967762742}\n",
      "tfidf : {'system': 0.5493061443340549, 'minors': 0.7520386983881371}\n"
     ]
    }
   ],
   "source": [
    "# 신규 문장의 특정 단어 TF-IDF 계산\n",
    "\n",
    "# 신규 문장\n",
    "new_sentence = \"system minors\"\n",
    "cleaned_words = filter.filter(text_cleaner.clean_text(tokenizer.tokenize(new_sentence)))\n",
    "print(cleaned_words)\n",
    "\n",
    "# TF-IDF 계산\n",
    "tfidf_model = StandardTFIDF(processed_corpus)\n",
    "tf = tfidf_model._tf(cleaned_words)\n",
    "idf = tfidf_model._idf(cleaned_words)\n",
    "tfidf = tfidf_model._tfidf(cleaned_words)\n",
    "print(f\"tf : {tf}\")\n",
    "print(f\"idf : {idf}\")\n",
    "print(f\"tfidf : {tfidf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd370b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, np.float64(0.5898341626740045)), (11, np.float64(0.8075244024440723))]\n"
     ]
    }
   ],
   "source": [
    "# gensim 라이브러리를 이용하는 경우\n",
    "\n",
    "from gensim import models\n",
    "\n",
    "# train tf-idf model from corpus\n",
    "bow_corpus = [bow_model.dictionary.doc2bow(doc) for doc in processed_corpus]\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "# test doc\n",
    "new_sentence = \"system minors\"\n",
    "cleaned_words = filter.filter(text_cleaner.clean_text(tokenizer.tokenize(new_sentence)))\n",
    "new_doc_bow = bow_model.dictionary.doc2bow(cleaned_words)\n",
    "new_doc_tfidf = tfidf[new_doc_bow]\n",
    "print(new_doc_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffdffbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['system', 'minors']\n",
      "tf : {'system': 0.5, 'minors': 0.5}\n",
      "idf : {'system': 1.916290731874155, 'minors': 2.203972804325936}\n",
      "tfidf : {'system': 0.9581453659370776, 'minors': 1.101986402162968}\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF (스무딩 적용)\n",
    "from math import log\n",
    "\n",
    "class SmoothingTFIDF(StandardTFIDF):\n",
    "    def __init__(self, corpus:list[list[str]]=None):\n",
    "        super().__init__(corpus)\n",
    "    \n",
    "    def _idf(self, document:list[str]) -> dict[str,int|float]:\n",
    "        idf = dict()\n",
    "        document_count = self.total_doc_count\n",
    "        cnt = Counter(document)\n",
    "        for token in cnt:\n",
    "            idf[token] = log((document_count + 1) / (self.document_frequency[token] + 1)) + 1\n",
    "        return idf\n",
    "\n",
    "# 신규 문장의 특정 단어 TF-IDF 계산\n",
    "\n",
    "# 신규 문장\n",
    "new_sentence = \"system minors\"\n",
    "cleaned_words = filter.filter(text_cleaner.clean_text(tokenizer.tokenize(new_sentence)))\n",
    "print(cleaned_words)\n",
    "\n",
    "# TF-IDF 계산\n",
    "tfidf_model = SmoothingTFIDF(processed_corpus)\n",
    "tf = tfidf_model._tf(cleaned_words)\n",
    "idf = tfidf_model._idf(cleaned_words)\n",
    "tfidf = tfidf_model._tfidf(cleaned_words)\n",
    "print(f\"tf : {tf}\")\n",
    "print(f\"idf : {idf}\")\n",
    "print(f\"tfidf : {tfidf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2133117",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_doc_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tf_system_compare = \u001b[38;5;28mlen\u001b[39m([word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnew_doc_words\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m word == \u001b[33m'\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      2\u001b[39m idf_system_compare = log((\u001b[38;5;28mlen\u001b[39m(processed_corpus)+\u001b[32m1\u001b[39m) / (\u001b[38;5;28mlen\u001b[39m([doc \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m processed_corpus \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m doc])+\u001b[32m1\u001b[39m)) + \u001b[32m1\u001b[39m\n\u001b[32m      3\u001b[39m tf_minors_compare = \u001b[38;5;28mlen\u001b[39m([word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m new_doc_words \u001b[38;5;28;01mif\u001b[39;00m word == \u001b[33m'\u001b[39m\u001b[33mminors\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'new_doc_words' is not defined"
     ]
    }
   ],
   "source": [
    "tf_system_compare = len([word for word in new_doc_words if word == 'system'])\n",
    "idf_system_compare = log((len(processed_corpus)+1) / (len([doc for doc in processed_corpus if 'system' in doc])+1)) + 1\n",
    "tf_minors_compare = len([word for word in new_doc_words if word == 'minors'])\n",
    "idf_minors_compare = log((len(processed_corpus)+1) / (len([doc for doc in processed_corpus if 'minors' in doc])+1)) + 1\n",
    "norm = ((tf_system_compare * idf_system_compare)**2 + (tf_minors_compare * idf_minors_compare)**2)**1/2\n",
    "print(norm)\n",
    "tfidf_system_compare = tf_system_compare * idf_system_compare\n",
    "print(f'word : system / id : {dictionary.token2id[\"system\"]} / tf : {tf_system_compare}, / idf : {idf_system_compare} / tfidf : {tfidf_system_compare/norm}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfe3d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfidf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
