{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64bf3acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63f77e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus\n",
    "\n",
    "text_corpus = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef7b83b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['human', 'interface', 'computer'],\n",
      " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'system'],\n",
      " ['system', 'human', 'system', 'eps'],\n",
      " ['user', 'response', 'time'],\n",
      " ['trees'],\n",
      " ['graph', 'trees'],\n",
      " ['graph', 'minors', 'trees'],\n",
      " ['graph', 'minors', 'survey']]\n"
     ]
    }
   ],
   "source": [
    "# preprocessing\n",
    "\n",
    "# Create a set of frequent words\n",
    "stoplist = set('for a of the and to in'.split(' '))\n",
    "\n",
    "# Lowercase each document, split it by white space and filter out stopwords\n",
    "texts = [[word for word in document.lower().split() if word not in stoplist]\n",
    "         for document in text_corpus]\n",
    "\n",
    "# Count word frequencies\n",
    "from collections import defaultdict\n",
    "frequency = defaultdict(int)\n",
    "for text in texts:\n",
    "    for token in text:\n",
    "        frequency[token] += 1\n",
    "\n",
    "# Only keep words that appear more than once\n",
    "processed_corpus = [[token for token in text if frequency[token] > 1] for text in texts]\n",
    "pprint.pprint(processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e692d011",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'computer': 0, 'human': 1, 'interface': 2, 'response': 3, 'survey': 4, 'system': 5, 'time': 6, 'user': 7, 'eps': 8, 'trees': 9, 'graph': 10, 'minors': 11}\n"
     ]
    }
   ],
   "source": [
    "# make dictionary\n",
    "\n",
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(processed_corpus)\n",
    "print(dictionary.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4dbc18d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BoW one document\n",
      "[(0, 1), (1, 1)]\n",
      "==================================================\n",
      "BoW test corpus\n",
      "[[(0, 1), (1, 1), (2, 1)],\n",
      " [(0, 1), (3, 1), (4, 1), (5, 1), (6, 1), (7, 1)],\n",
      " [(2, 1), (5, 1), (7, 1), (8, 1)],\n",
      " [(1, 1), (5, 2), (8, 1)],\n",
      " [(3, 1), (6, 1), (7, 1)],\n",
      " [(9, 1)],\n",
      " [(9, 1), (10, 1)],\n",
      " [(9, 1), (10, 1), (11, 1)],\n",
      " [(4, 1), (10, 1), (11, 1)]]\n"
     ]
    }
   ],
   "source": [
    "# doc 2 bow\n",
    "\n",
    "# just one test\n",
    "new_doc = \"Human computer interaction\"\n",
    "bow_new_doc = dictionary.doc2bow(new_doc.lower().split())\n",
    "print(\"BoW one document\")\n",
    "print(bow_new_doc)\n",
    "print(\"=\"*50)\n",
    "\n",
    "# corpus\n",
    "bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "print(\"BoW test corpus\")\n",
    "pprint.pprint(bow_corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042ddc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "words of corpus\n",
      "[['human', 'interface', 'computer'],\n",
      " ['survey', 'user', 'computer', 'system', 'response', 'time'],\n",
      " ['eps', 'user', 'interface', 'system'],\n",
      " ['system', 'human', 'system', 'eps'],\n",
      " ['user', 'response', 'time'],\n",
      " ['trees'],\n",
      " ['graph', 'trees'],\n",
      " ['graph', 'minors', 'trees'],\n",
      " ['graph', 'minors', 'survey']]\n",
      "==================================================\n",
      "new doc words\n",
      "\n",
      "['system', 'minors']\n",
      "new doc BoW\n",
      "[(5, 1), (11, 1)]\n",
      "new doc TF-IDF\n",
      "[(5, np.float64(0.5898341626740045)), (11, np.float64(0.8075244024440723))]\n",
      "==================================================\n",
      "손코딩 검증\n",
      "word : system / id : 5 / tf : 0.5, / idf : 1.0986122886681098 / tfidf : 0.5493061443340549\n",
      "word : minors / id : 11 / tf : 0.5, / idf : 1.5040773967762742 / tfidf : 0.7520386983881371\n",
      "==================================================\n",
      "gensim 은 tf를 단순 해당 단어의 출현 횟수로 계산하며\n",
      "gensim 은 idf를 log(문서수+1/단어출현문서수+1) + 1 로 계산하여 0이 되는 것을 방지한다.\n",
      "또한 gensim은 최종적으로 문서 길이 정규화(L2norm)를 수행해 벡터의 유클리드 거리가 1이 되도록 한다\n",
      "4.264833145637557\n",
      "word : system / id : 5 / tf : 1, / idf : 1.916290731874155 / tfidf : 0.4493237288390295\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF (손코딩)\n",
    "from math import log\n",
    "\n",
    "# test doc\n",
    "new_doc_words = \"system minors\".lower().split()\n",
    "new_doc_bow = dictionary.doc2bow(new_doc_words)\n",
    "print(\"words of corpus\")\n",
    "pprint.pprint(processed_corpus)\n",
    "print('='*50)\n",
    "print(\"new doc words\")\n",
    "print(new_doc_words)\n",
    "print(\"new doc BoW\")\n",
    "print(new_doc_bow)\n",
    "print(\"new doc TF-IDF\")\n",
    "\n",
    "# 손코딩 검증\n",
    "tf_system = len([word for word in new_doc_words if word == 'system']) / len(new_doc_words)\n",
    "idf_system = log(len(processed_corpus) / len([doc for doc in processed_corpus if 'system' in doc]))\n",
    "tfidf_system = tf_system * idf_system\n",
    "tf_minors = len([word for word in new_doc_words if word == 'minors']) / len(new_doc_words)\n",
    "idf_minors = log(len(processed_corpus) / len([doc for doc in processed_corpus if 'minors' in doc]))\n",
    "tfidf_minors = tf_minors * idf_minors\n",
    "print(\"손코딩 검증 / log는 자연로그(ln) 사용\")\n",
    "print(f'word : system / id : {dictionary.token2id[\"system\"]} / tf : {tf_system}, / idf : {idf_system} / tfidf : {tfidf_system}')\n",
    "print(f'word : minors / id : {dictionary.token2id[\"minors\"]} / tf : {tf_minors}, / idf : {idf_minors} / tfidf : {tfidf_minors}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf1ab4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, np.float64(0.5898341626740045)), (11, np.float64(0.8075244024440723))]\n"
     ]
    }
   ],
   "source": [
    "from gensim import models\n",
    "\n",
    "# train tf-idf model from corpus\n",
    "tfidf = models.TfidfModel(bow_corpus) # bow_corpus : test_corpus 를 BoW 한 리스트\n",
    "\n",
    "# test doc\n",
    "new_doc_words = \"system minors\".lower().split()\n",
    "new_doc_tfidf = tfidf[new_doc_bow]\n",
    "print(new_doc_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffdffbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gensim 라이브러리와 손코딩 내용이 다른 이유\n",
    "print('='*50)\n",
    "print('gensim 은 tf를 단순 해당 단어의 출현 횟수로 계산하며')\n",
    "print('gensim 은 idf를 log(문서수+1/단어출현문서수+1) + 1 로 계산하여 0이 되는 것을 방지한다.')\n",
    "print('또한 gensim은 최종적으로 문서 길이 정규화(L2norm)를 수행해 벡터의 유클리드 거리가 1이 되도록 한다')\n",
    "tf_system_compare = len([word for word in new_doc_words if word == 'system'])\n",
    "idf_system_compare = log((len(processed_corpus)+1) / (len([doc for doc in processed_corpus if 'system' in doc])+1)) + 1\n",
    "tf_minors_compare = len([word for word in new_doc_words if word == 'minors'])\n",
    "idf_minors_compare = log((len(processed_corpus)+1) / (len([doc for doc in processed_corpus if 'minors' in doc])+1)) + 1\n",
    "norm = ((tf_system_compare * idf_system_compare)**2 + (tf_minors_compare * idf_minors_compare)**2)**1/2\n",
    "print(norm)\n",
    "tfidf_system_compare = tf_system_compare * idf_system_compare\n",
    "print(f'word : system / id : {dictionary.token2id[\"system\"]} / tf : {tf_system_compare}, / idf : {idf_system_compare} / tfidf : {tfidf_system_compare/norm}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfidf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
