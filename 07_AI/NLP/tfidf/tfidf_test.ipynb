{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64bf3acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63f77e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus\n",
    "\n",
    "text_corpus = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ef7b83b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "class WhitespaceTokenizer():\n",
    "    def tokenize(self, input:str) -> list[str]:\n",
    "        if isinstance(input, str):\n",
    "            result = input.split(\" \")\n",
    "        return result\n",
    "\n",
    "# Text Cleaner\n",
    "class TextCleaner:\n",
    "    def __init__(self):\n",
    "        # Create a set of frequent words\n",
    "        self.stopwords = set('for a of the and to in'.split(' '))\n",
    "    def clean_text(self, words:list[str]) -> list[str]:\n",
    "        # Lowercase each document, split it by white space and filter out stopwords\n",
    "        words = [word.lower() for word in words if word.lower() not in self.stopwords]\n",
    "        return words\n",
    "\n",
    "# filter by frequency\n",
    "class FilterByFrequency:\n",
    "    def __init__(self):\n",
    "        # Count word frequencies\n",
    "        from collections import defaultdict\n",
    "        self.frequency_dict = defaultdict(int)\n",
    "    def make_filter(self, docs:list[list[str]]):\n",
    "        for text in docs:\n",
    "            for token in text:\n",
    "                self.frequency_dict[token] += 1\n",
    "    def filter(self, words:list[str], threshold:int=1):\n",
    "        # Only keep words that appear more than once\n",
    "        filtered_words = [token for token in words if self.frequency_dict[token] > threshold]\n",
    "        return filtered_words\n",
    "\n",
    "# bow\n",
    "class BagOfWords:\n",
    "    def __init__(self):\n",
    "        self.dictionary:dict[str,int]|None=None\n",
    "    def create_dictionary(self, input:list[list[str]]):\n",
    "        from gensim import corpora\n",
    "        self.dictionary = corpora.Dictionary(input)\n",
    "    def represent_bow(self, input:list[list[str]]):\n",
    "        bow_corpus = [self.dictionary.doc2bow(text) for text in input]\n",
    "        return bow_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e692d011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) 토크나이징 : 공백을 기준으로\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "tokenized_docs = [tokenizer.tokenize(doc) for doc in text_corpus]\n",
    "# (2) 텍스트 클리닝 - lower + stopwords\n",
    "text_cleaner = TextCleaner()\n",
    "cleaned_docs = [text_cleaner.clean_text(words) for words in tokenized_docs]\n",
    "# (3) 빈도 기반 필터링 : 1회 발생 단어는 제외\n",
    "filter = FilterByFrequency()\n",
    "filter.make_filter(cleaned_docs)\n",
    "processed_corpus = [filter.filter(doc, 1) for doc in cleaned_docs]\n",
    "# (4) BoW 생성\n",
    "bow_model = BagOfWords()\n",
    "bow_model.create_dictionary(processed_corpus)\n",
    "bow = bow_model.represent_bow(processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c16ac09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF (전통적 TF-IDF)\n",
    "from math import log\n",
    "\n",
    "class StandardTFIDF:\n",
    "    def __init__(self, Documents):\n",
    "        self.Documents:list[list[str]] = Documents\n",
    "    def tf(self, document:list[str]):\n",
    "        result = []\n",
    "        all_word_count = len(document)\n",
    "        for word in document:\n",
    "            target_word_count = len([dw for dw in document if dw == word])\n",
    "            result.append((word, target_word_count/all_word_count))\n",
    "        return result\n",
    "    def idf(self, document:list[str]):\n",
    "        result = []\n",
    "        document_count = len(self.Documents)\n",
    "        for word in document:\n",
    "            include_word_document_count = len([doc for doc in self.Documents if word in doc])\n",
    "            result.append((word, log(document_count/include_word_document_count)))\n",
    "        return result\n",
    "    def tfidf(self, document:list[str]):\n",
    "        result = []\n",
    "        for tf, idf in zip(self.tf(document), self.idf(document)):\n",
    "            result.append((tf[0], tf[1] * idf[1]))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "01da202b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['system', 'minors']\n",
      "tf : [('system', 0.5), ('minors', 0.5)]\n",
      "idf : [('system', 1.0986122886681098), ('minors', 1.5040773967762742)]\n",
      "tfidf : [('system', 0.5493061443340549), ('minors', 0.7520386983881371)]\n"
     ]
    }
   ],
   "source": [
    "# 신규 문장의 특정 단어 TF-IDF 계산\n",
    "\n",
    "# 신규 문장\n",
    "new_sentence = \"system minors\"\n",
    "cleaned_words = filter.filter(text_cleaner.clean_text(tokenizer.tokenize(new_sentence)))\n",
    "print(cleaned_words)\n",
    "\n",
    "# TF-IDF 계산\n",
    "tfidf_model = StandardTFIDF(processed_corpus)\n",
    "tf = tfidf_model.tf(cleaned_words)\n",
    "idf = tfidf_model.idf(cleaned_words)\n",
    "tfidf = tfidf_model.tfidf(cleaned_words)\n",
    "print(f\"tf : {tf}\")\n",
    "print(f\"idf : {idf}\")\n",
    "print(f\"tfidf : {tfidf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bd370b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, np.float64(0.5898341626740045)), (11, np.float64(0.8075244024440723))]\n"
     ]
    }
   ],
   "source": [
    "# gensim 라이브러리를 이용하는 경우\n",
    "\n",
    "from gensim import models\n",
    "\n",
    "# train tf-idf model from corpus\n",
    "bow_corpus = [bow_model.dictionary.doc2bow(doc) for doc in processed_corpus]\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "# test doc\n",
    "new_sentence = \"system minors\"\n",
    "cleaned_words = filter.filter(text_cleaner.clean_text(tokenizer.tokenize(new_sentence)))\n",
    "new_doc_bow = bow_model.dictionary.doc2bow(cleaned_words)\n",
    "new_doc_tfidf = tfidf[new_doc_bow]\n",
    "print(new_doc_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ffdffbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['system', 'minors']\n",
      "tf : [('system', 0.5), ('minors', 0.5)]\n",
      "idf : [('system', 1.916290731874155), ('minors', 2.203972804325936)]\n",
      "tfidf : [('system', 0.9581453659370776), ('minors', 1.101986402162968)]\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF (스무딩 적용)\n",
    "from math import log\n",
    "\n",
    "class SmoothingTFIDF:\n",
    "    def __init__(self, Documents):\n",
    "        self.Documents:list[list[str]] = Documents\n",
    "    def tf(self, document:list[str]):\n",
    "        result = []\n",
    "        all_word_count = len(document)\n",
    "        for word in document:\n",
    "            target_word_count = len([dw for dw in document if dw == word])\n",
    "            result.append((word, target_word_count/all_word_count))\n",
    "        return result\n",
    "    def idf(self, document:list[str]):\n",
    "        result = []\n",
    "        document_count = len(self.Documents) + 1\n",
    "        for word in document:\n",
    "            include_word_document_count = len([doc for doc in self.Documents if word in doc]) + 1\n",
    "            result.append((word, log(document_count/include_word_document_count) + 1))\n",
    "        return result\n",
    "    def tfidf(self, document:list[str]):\n",
    "        result = []\n",
    "        for tf, idf in zip(self.tf(document), self.idf(document)):\n",
    "            result.append((tf[0], tf[1] * idf[1]))\n",
    "        return result\n",
    "\n",
    "# 신규 문장의 특정 단어 TF-IDF 계산\n",
    "\n",
    "# 신규 문장\n",
    "new_sentence = \"system minors\"\n",
    "cleaned_words = filter.filter(text_cleaner.clean_text(tokenizer.tokenize(new_sentence)))\n",
    "print(cleaned_words)\n",
    "\n",
    "# TF-IDF 계산\n",
    "tfidf_model = SmoothingTFIDF(processed_corpus)\n",
    "tf = tfidf_model.tf(cleaned_words)\n",
    "idf = tfidf_model.idf(cleaned_words)\n",
    "tfidf = tfidf_model.tfidf(cleaned_words)\n",
    "print(f\"tf : {tf}\")\n",
    "print(f\"idf : {idf}\")\n",
    "print(f\"tfidf : {tfidf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2133117",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'new_doc_words' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m tf_system_compare = \u001b[38;5;28mlen\u001b[39m([word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m \u001b[43mnew_doc_words\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m word == \u001b[33m'\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m'\u001b[39m])\n\u001b[32m      2\u001b[39m idf_system_compare = log((\u001b[38;5;28mlen\u001b[39m(processed_corpus)+\u001b[32m1\u001b[39m) / (\u001b[38;5;28mlen\u001b[39m([doc \u001b[38;5;28;01mfor\u001b[39;00m doc \u001b[38;5;129;01min\u001b[39;00m processed_corpus \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m'\u001b[39m\u001b[33msystem\u001b[39m\u001b[33m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m doc])+\u001b[32m1\u001b[39m)) + \u001b[32m1\u001b[39m\n\u001b[32m      3\u001b[39m tf_minors_compare = \u001b[38;5;28mlen\u001b[39m([word \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m new_doc_words \u001b[38;5;28;01mif\u001b[39;00m word == \u001b[33m'\u001b[39m\u001b[33mminors\u001b[39m\u001b[33m'\u001b[39m])\n",
      "\u001b[31mNameError\u001b[39m: name 'new_doc_words' is not defined"
     ]
    }
   ],
   "source": [
    "tf_system_compare = len([word for word in new_doc_words if word == 'system'])\n",
    "idf_system_compare = log((len(processed_corpus)+1) / (len([doc for doc in processed_corpus if 'system' in doc])+1)) + 1\n",
    "tf_minors_compare = len([word for word in new_doc_words if word == 'minors'])\n",
    "idf_minors_compare = log((len(processed_corpus)+1) / (len([doc for doc in processed_corpus if 'minors' in doc])+1)) + 1\n",
    "norm = ((tf_system_compare * idf_system_compare)**2 + (tf_minors_compare * idf_minors_compare)**2)**1/2\n",
    "print(norm)\n",
    "tfidf_system_compare = tf_system_compare * idf_system_compare\n",
    "print(f'word : system / id : {dictionary.token2id[\"system\"]} / tf : {tf_system_compare}, / idf : {idf_system_compare} / tfidf : {tfidf_system_compare/norm}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddfe3d7d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfidf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
