{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64bf3acf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "63f77e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus\n",
    "\n",
    "text_corpus = [\n",
    "    \"Human machine interface for lab abc computer applications\",\n",
    "    \"A survey of user opinion of computer system response time\",\n",
    "    \"The EPS user interface management system\",\n",
    "    \"System and human system engineering testing of EPS\",\n",
    "    \"Relation of user perceived response time to error measurement\",\n",
    "    \"The generation of random binary unordered trees\",\n",
    "    \"The intersection graph of paths in trees\",\n",
    "    \"Graph minors IV Widths of trees and well quasi ordering\",\n",
    "    \"Graph minors A survey\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef7b83b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer\n",
    "class WhitespaceTokenizer():\n",
    "    def tokenize(self, input:str) -> list[str]:\n",
    "        if isinstance(input, str):\n",
    "            result = input.split(\" \")\n",
    "        return result\n",
    "\n",
    "# Text Cleaner\n",
    "class TextCleaner:\n",
    "    def __init__(self):\n",
    "        # Create a set of frequent words\n",
    "        self.stopwords = set('for a of the and to in'.split(' '))\n",
    "    def clean_text(self, words:list[str]) -> list[str]:\n",
    "        # Lowercase each document, split it by white space and filter out stopwords\n",
    "        words = [word.lower() for word in words if word.lower() not in self.stopwords]\n",
    "        return words\n",
    "\n",
    "# filter by frequency\n",
    "class FilterByFrequency:\n",
    "    def __init__(self):\n",
    "        # Count word frequencies\n",
    "        from collections import defaultdict\n",
    "        self.frequency_dict = defaultdict(int)\n",
    "    def make_filter(self, docs:list[list[str]]):\n",
    "        for text in docs:\n",
    "            for token in text:\n",
    "                self.frequency_dict[token] += 1\n",
    "    def filter(self, words:list[str], threshold:int=1):\n",
    "        # Only keep words that appear more than once\n",
    "        filtered_words = [token for token in words if self.frequency_dict[token] > threshold]\n",
    "        return filtered_words\n",
    "\n",
    "# bow\n",
    "class BagOfWords:\n",
    "    def __init__(self):\n",
    "        self.dictionary:dict[str,int]|None=None\n",
    "    def create_dictionary(self, input:list[list[str]]):\n",
    "        from gensim import corpora\n",
    "        self.dictionary = corpora.Dictionary(input)\n",
    "    def represent_bow(self, input:list[list[str]]):\n",
    "        bow_corpus = [self.dictionary.doc2bow(text) for text in input]\n",
    "        return bow_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e692d011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# (1) 토크나이징 : 공백을 기준으로\n",
    "tokenizer = WhitespaceTokenizer()\n",
    "tokenized_docs = [tokenizer.tokenize(doc) for doc in text_corpus]\n",
    "# (2) 텍스트 클리닝 - lower + stopwords\n",
    "text_cleaner = TextCleaner()\n",
    "cleaned_docs = [text_cleaner.clean_text(words) for words in tokenized_docs]\n",
    "# (3) 빈도 기반 필터링 : 1회 발생 단어는 제외\n",
    "filter = FilterByFrequency()\n",
    "filter.make_filter(cleaned_docs)\n",
    "processed_corpus = [filter.filter(doc, 1) for doc in cleaned_docs]\n",
    "# (4) BoW 생성\n",
    "bow_model = BagOfWords()\n",
    "bow_model.create_dictionary(processed_corpus)\n",
    "bow = bow_model.represent_bow(processed_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "904f4370",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF (손코딩)\n",
    "from math import log\n",
    "\n",
    "class CustomTFIDF:\n",
    "    def __init__(self, Documents):\n",
    "        self.Documents:list[list[str]] = Documents\n",
    "    def tf(self, word, document):\n",
    "        target_word_count = len([dw for dw in document if dw == word])\n",
    "        all_word_count = len(document)\n",
    "        return target_word_count/all_word_count\n",
    "    def idf(self, word):\n",
    "        document_count = len(self.Documents)\n",
    "        include_word_document_count = len([doc for doc in self.Documents if word in doc])\n",
    "        return log(document_count/include_word_document_count)\n",
    "    def tfidf(self, word, document):\n",
    "        return self.tf(word, document) * self.idf(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f748fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['system', 'minors']\n",
      "===== system =====\n",
      "system's tf : 0.5\n",
      "idf : 1.0986122886681098\n",
      "tfidf : 0.5493061443340549\n",
      "===== minors =====\n",
      "minors's tf : 0.5\n",
      "idf : 1.5040773967762742\n",
      "tfidf : 0.7520386983881371\n"
     ]
    }
   ],
   "source": [
    "# 신규 문장의 특정 단어 TF-IDF 계산\n",
    "\n",
    "# 신규 문장\n",
    "new_sentence = \"system minors\"\n",
    "cleaned_words = filter.filter(text_cleaner.clean_text(tokenizer.tokenize(new_sentence)))\n",
    "print(cleaned_words)\n",
    "\n",
    "# TF-IDF 계산\n",
    "tfidf_model = CustomTFIDF(processed_corpus)\n",
    "for word in cleaned_words:\n",
    "    print(f\"===== {word} =====\")\n",
    "    tf = tfidf_model.tf(word=word, document=cleaned_words)\n",
    "    idf = tfidf_model.idf(word=word)\n",
    "    tfidf = tfidf_model.tfidf(word=word, document=cleaned_words)\n",
    "    print(f\"{word}'s tf : {tf}\\nidf : {idf}\\ntfidf : {tfidf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd370b56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(5, np.float64(0.5898341626740045)), (11, np.float64(0.8075244024440723))]\n"
     ]
    }
   ],
   "source": [
    "# gensim 라이브러리를 이용하는 경우\n",
    "\n",
    "from gensim import models\n",
    "\n",
    "# train tf-idf model from corpus\n",
    "bow_corpus = [bow_model.dictionary.doc2bow(doc) for doc in processed_corpus]\n",
    "tfidf = models.TfidfModel(bow_corpus)\n",
    "\n",
    "# test doc\n",
    "new_sentence = \"system minors\"\n",
    "cleaned_words = filter.filter(text_cleaner.clean_text(tokenizer.tokenize(new_sentence)))\n",
    "new_doc_bow = bow_model.dictionary.doc2bow(cleaned_words)\n",
    "new_doc_tfidf = tfidf[new_doc_bow]\n",
    "print(new_doc_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1ffdffbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['system', 'minors']\n",
      "===== system =====\n",
      "system's tf : 0.5\n",
      "idf : 1.916290731874155\n",
      "tfidf : 0.9581453659370776\n",
      "===== minors =====\n",
      "minors's tf : 0.5\n",
      "idf : 2.203972804325936\n",
      "tfidf : 1.101986402162968\n"
     ]
    }
   ],
   "source": [
    "# 스무딩 적용 TF-IDF\n",
    "from math import log\n",
    "class SmoothingTFIDF:\n",
    "    def __init__(self, Documents):\n",
    "        self.Documents:list[list[str]] = Documents\n",
    "    def tf(self, word, document):\n",
    "        target_word_count = len([dw for dw in document if dw == word])\n",
    "        all_word_count = len(document)\n",
    "        return target_word_count/all_word_count\n",
    "    def idf(self, word):\n",
    "        document_count = len(self.Documents) + 1\n",
    "        include_word_document_count = len([doc for doc in self.Documents if word in doc]) + 1\n",
    "        return log(document_count/include_word_document_count) + 1\n",
    "    def tfidf(self, word, document):\n",
    "        return self.tf(word, document) * self.idf(word)\n",
    "\n",
    "# 신규 문장의 특정 단어 TF-IDF 계산\n",
    "\n",
    "# 신규 문장\n",
    "new_sentence = \"system minors\"\n",
    "cleaned_words = filter.filter(text_cleaner.clean_text(tokenizer.tokenize(new_sentence)))\n",
    "print(cleaned_words)\n",
    "\n",
    "# TF-IDF 계산\n",
    "tfidf_model = SmoothingTFIDF(processed_corpus)\n",
    "for word in cleaned_words:\n",
    "    print(f\"===== {word} =====\")\n",
    "    tf = tfidf_model.tf(word=word, document=cleaned_words)\n",
    "    idf = tfidf_model.idf(word=word)\n",
    "    tfidf = tfidf_model.tfidf(word=word, document=cleaned_words)\n",
    "    print(f\"{word}'s tf : {tf}\\nidf : {idf}\\ntfidf : {tfidf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2133117",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_system_compare = len([word for word in new_doc_words if word == 'system'])\n",
    "idf_system_compare = log((len(processed_corpus)+1) / (len([doc for doc in processed_corpus if 'system' in doc])+1)) + 1\n",
    "tf_minors_compare = len([word for word in new_doc_words if word == 'minors'])\n",
    "idf_minors_compare = log((len(processed_corpus)+1) / (len([doc for doc in processed_corpus if 'minors' in doc])+1)) + 1\n",
    "norm = ((tf_system_compare * idf_system_compare)**2 + (tf_minors_compare * idf_minors_compare)**2)**1/2\n",
    "print(norm)\n",
    "tfidf_system_compare = tf_system_compare * idf_system_compare\n",
    "print(f'word : system / id : {dictionary.token2id[\"system\"]} / tf : {tf_system_compare}, / idf : {idf_system_compare} / tfidf : {tfidf_system_compare/norm}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
