{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca904a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "#\n",
    "# Copyright (C) 2012 Radim Rehurek <radimrehurek@seznam.cz>\n",
    "# Copyright (C) 2017 Mohit Rathore <mrmohitrathoremr@gmail.com>\n",
    "# Licensed under the GNU LGPL v2.1 - https://www.gnu.org/licenses/old-licenses/lgpl-2.1.en.html\n",
    "\n",
    "\"\"\"This module implements functionality related to the `Term Frequency - Inverse Document Frequency\n",
    "<https://en.wikipedia.org/wiki/Tf%E2%80%93idf>`_ class of bag-of-words vector space models.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "from functools import partial\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from gensim import interfaces, matutils, utils\n",
    "from gensim.utils import deprecated\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def resolve_weights(smartirs):\n",
    "    \"\"\"Check the validity of `smartirs` parameters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    smartirs : str\n",
    "        `smartirs` or SMART (System for the Mechanical Analysis and Retrieval of Text)\n",
    "        Information Retrieval System, a mnemonic scheme for denoting tf-idf weighting\n",
    "        variants in the vector space model. The mnemonic for representing a combination\n",
    "        of weights takes the form ddd, where the letters represents the term weighting of the document vector.\n",
    "        for more information visit `SMART Information Retrieval System\n",
    "        <https://en.wikipedia.org/wiki/SMART_Information_Retrieval_System>`_.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str of (local_letter, global_letter, normalization_letter)\n",
    "\n",
    "    local_letter : str\n",
    "        Term frequency weighing, one of:\n",
    "            * `b` - binary,\n",
    "            * `t` or `n` - raw,\n",
    "            * `a` - augmented,\n",
    "            * `l` - logarithm,\n",
    "            * `d` - double logarithm,\n",
    "            * `L` - log average.\n",
    "    global_letter : str\n",
    "        Document frequency weighting, one of:\n",
    "            * `x` or `n` - none,\n",
    "            * `f` - idf,\n",
    "            * `t` - zero-corrected idf,\n",
    "            * `p` - probabilistic idf.\n",
    "    normalization_letter : str\n",
    "        Document normalization, one of:\n",
    "            * `x` or `n` - none,\n",
    "            * `c` - cosine,\n",
    "            * `u` - pivoted unique,\n",
    "            * `b` - pivoted character length.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    ValueError\n",
    "        If `smartirs` is not a string of length 3 or one of the decomposed value\n",
    "        doesn't fit the list of permissible values.\n",
    "    \"\"\"\n",
    "    if isinstance(smartirs, str) and re.match(r\"...\\....\", smartirs):\n",
    "        match = re.match(r\"(?P<ddd>...)\\.(?P<qqq>...)\", smartirs)\n",
    "        raise ValueError(\n",
    "            \"The notation {ddd}.{qqq} specifies two term-weighting schemes, \"\n",
    "            \"one for collection documents ({ddd}) and one for queries ({qqq}). \"\n",
    "            \"You must train two separate tf-idf models.\".format(\n",
    "                ddd=match.group(\"ddd\"),\n",
    "                qqq=match.group(\"qqq\"),\n",
    "            )\n",
    "        )\n",
    "    if not isinstance(smartirs, str) or len(smartirs) != 3:\n",
    "        raise ValueError(\"Expected a string of length 3 got \" + smartirs)\n",
    "\n",
    "    w_tf, w_df, w_n = smartirs\n",
    "\n",
    "    if w_tf not in 'btnaldL':\n",
    "        raise ValueError(\"Expected term frequency weight to be one of 'btnaldL', got {}\".format(w_tf))\n",
    "\n",
    "    if w_df not in 'xnftp':\n",
    "        raise ValueError(\"Expected inverse document frequency weight to be one of 'xnftp', got {}\".format(w_df))\n",
    "\n",
    "    if w_n not in 'xncub':\n",
    "        raise ValueError(\"Expected normalization weight to be one of 'xncub', got {}\".format(w_n))\n",
    "\n",
    "    # resolve aliases\n",
    "    if w_tf == \"t\":\n",
    "        w_tf = \"n\"\n",
    "    if w_df == \"x\":\n",
    "        w_df = \"n\"\n",
    "    if w_n == \"x\":\n",
    "        w_n = \"n\"\n",
    "\n",
    "    return w_tf + w_df + w_n\n",
    "\n",
    "\n",
    "def df2idf(docfreq, totaldocs, log_base=2.0, add=0.0):\n",
    "    r\"\"\"Compute inverse-document-frequency for a term with the given document frequency `docfreq`:\n",
    "    :math:`idf = add + log_{log\\_base} \\frac{totaldocs}{docfreq}`\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    docfreq : {int, float}\n",
    "        Document frequency.\n",
    "    totaldocs : int\n",
    "        Total number of documents.\n",
    "    log_base : float, optional\n",
    "        Base of logarithm.\n",
    "    add : float, optional\n",
    "        Offset.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Inverse document frequency.\n",
    "\n",
    "    \"\"\"\n",
    "    return add + np.log(float(totaldocs) / docfreq) / np.log(log_base)\n",
    "\n",
    "\n",
    "def precompute_idfs(wglobal, dfs, total_docs):\n",
    "    \"\"\"Pre-compute the inverse document frequency mapping for all terms.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    wglobal : function\n",
    "        Custom function for calculating the \"global\" weighting function.\n",
    "        See for example the SMART alternatives under :func:`~gensim.models.tfidfmodel.smartirs_wglobal`.\n",
    "    dfs : dict\n",
    "        Dictionary mapping `term_id` into how many documents did that term appear in.\n",
    "    total_docs : int\n",
    "        Total number of documents.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict of (int, float)\n",
    "        Inverse document frequencies in the format `{term_id_1: idfs_1, term_id_2: idfs_2, ...}`.\n",
    "\n",
    "    \"\"\"\n",
    "    # not strictly necessary and could be computed on the fly in TfidfModel__getitem__.\n",
    "    # this method is here just to speed things up a little.\n",
    "    return {termid: wglobal(df, total_docs) for termid, df in dfs.items()}\n",
    "\n",
    "\n",
    "def smartirs_wlocal(tf, local_scheme):\n",
    "    \"\"\"Calculate local term weight for a term using the weighting scheme specified in `local_scheme`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tf : int\n",
    "        Term frequency.\n",
    "    local : {'b', 'n', 'a', 'l', 'd', 'L'}\n",
    "        Local transformation scheme.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Calculated local weight.\n",
    "\n",
    "    \"\"\"\n",
    "    if local_scheme == \"n\":\n",
    "        return tf\n",
    "    elif local_scheme == \"l\":\n",
    "        return 1 + np.log2(tf)\n",
    "    elif local_scheme == \"d\":\n",
    "        return 1 + np.log2(1 + np.log2(tf))\n",
    "    elif local_scheme == \"a\":\n",
    "        return 0.5 + (0.5 * tf / tf.max(axis=0))\n",
    "    elif local_scheme == \"b\":\n",
    "        return tf.astype('bool').astype('int')\n",
    "    elif local_scheme == \"L\":\n",
    "        return (1 + np.log2(tf)) / (1 + np.log2(tf.mean(axis=0)))\n",
    "\n",
    "\n",
    "def smartirs_wglobal(docfreq, totaldocs, global_scheme):\n",
    "    \"\"\"Calculate global document weight based on the weighting scheme specified in `global_scheme`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    docfreq : int\n",
    "        Document frequency.\n",
    "    totaldocs : int\n",
    "        Total number of documents.\n",
    "    global_scheme : {'n', 'f', 't', 'p'}\n",
    "        Global transformation scheme.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    float\n",
    "        Calculated global weight.\n",
    "\n",
    "    \"\"\"\n",
    "    if global_scheme == \"n\":\n",
    "        return 1.0\n",
    "    elif global_scheme == \"f\":\n",
    "        return np.log2(1.0 * totaldocs / docfreq)\n",
    "    elif global_scheme == \"t\":\n",
    "        return np.log2((totaldocs + 1.0) / docfreq)\n",
    "    elif global_scheme == \"p\":\n",
    "        return max(0, np.log2((1.0 * totaldocs - docfreq) / docfreq))\n",
    "\n",
    "\n",
    "@deprecated(\"Function will be removed in 4.0.0\")\n",
    "def smartirs_normalize(x, norm_scheme, return_norm=False):\n",
    "    \"\"\"Normalize a vector using the normalization scheme specified in `norm_scheme`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : numpy.ndarray\n",
    "        The tf-idf vector.\n",
    "    norm_scheme : {'n', 'c'}\n",
    "        Document length normalization scheme.\n",
    "    return_norm : bool, optional\n",
    "        Return the length of `x` as well?\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        Normalized array.\n",
    "    float (only if return_norm is set)\n",
    "        Norm of `x`.\n",
    "    \"\"\"\n",
    "    if norm_scheme == \"n\":\n",
    "        if return_norm:\n",
    "            _, length = matutils.unitvec(x, return_norm=return_norm)\n",
    "            return x, length\n",
    "        else:\n",
    "            return x\n",
    "    elif norm_scheme == \"c\":\n",
    "        return matutils.unitvec(x, return_norm=return_norm)\n",
    "\n",
    "\n",
    "class TfidfModel(interfaces.TransformationABC):\n",
    "    \"\"\"Objects of this class realize the transformation between word-document co-occurrence matrix (int)\n",
    "    into a locally/globally weighted TF-IDF matrix (positive floats).\n",
    "\n",
    "    Examples\n",
    "    --------\n",
    "    .. sourcecode:: pycon\n",
    "\n",
    "        >>> import gensim.downloader as api\n",
    "        >>> from gensim.models import TfidfModel\n",
    "        >>> from gensim.corpora import Dictionary\n",
    "        >>>\n",
    "        >>> dataset = api.load(\"text8\")\n",
    "        >>> dct = Dictionary(dataset)  # fit dictionary\n",
    "        >>> corpus = [dct.doc2bow(line) for line in dataset]  # convert corpus to BoW format\n",
    "        >>>\n",
    "        >>> model = TfidfModel(corpus)  # fit model\n",
    "        >>> vector = model[corpus[0]]  # apply model to the first corpus document\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, corpus=None, id2word=None, dictionary=None, wlocal=utils.identity,\n",
    "                 wglobal=df2idf, normalize=True, smartirs=None, pivot=None, slope=0.25):\n",
    "        r\"\"\"Compute TF-IDF by multiplying a local component (term frequency) with a global component\n",
    "        (inverse document frequency), and normalizing the resulting documents to unit length.\n",
    "        Formula for non-normalized weight of term :math:`i` in document :math:`j` in a corpus of :math:`D` documents\n",
    "\n",
    "        .. math:: weight_{i,j} = frequency_{i,j} * log_2 \\frac{D}{document\\_freq_{i}}\n",
    "\n",
    "        or, more generally\n",
    "\n",
    "        .. math:: weight_{i,j} = wlocal(frequency_{i,j}) * wglobal(document\\_freq_{i}, D)\n",
    "\n",
    "        so you can plug in your own custom :math:`wlocal` and :math:`wglobal` functions.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        corpus : iterable of iterable of (int, int), optional\n",
    "            Input corpus\n",
    "        id2word : {dict, :class:`~gensim.corpora.Dictionary`}, optional\n",
    "            Mapping token - id, that was used for converting input data to bag of words format.\n",
    "        dictionary : :class:`~gensim.corpora.Dictionary`\n",
    "            If `dictionary` is specified, it must be a `corpora.Dictionary` object and it will be used.\n",
    "            to directly construct the inverse document frequency mapping (then `corpus`, if specified, is ignored).\n",
    "        wlocals : callable, optional\n",
    "            Function for local weighting, default for `wlocal` is :func:`~gensim.utils.identity`\n",
    "            (other options: :func:`numpy.sqrt`, `lambda tf: 0.5 + (0.5 * tf / tf.max())`, etc.).\n",
    "        wglobal : callable, optional\n",
    "            Function for global weighting, default is :func:`~gensim.models.tfidfmodel.df2idf`.\n",
    "        normalize : {bool, callable}, optional\n",
    "            Normalize document vectors to unit euclidean length? You can also inject your own function into `normalize`.\n",
    "        smartirs : str, optional\n",
    "            SMART (System for the Mechanical Analysis and Retrieval of Text) Information Retrieval System,\n",
    "            a mnemonic scheme for denoting tf-idf weighting variants in the vector space model.\n",
    "            The mnemonic for representing a combination of weights takes the form XYZ,\n",
    "            for example 'ntc', 'bpn' and so on, where the letters represents the term weighting of the document vector.\n",
    "\n",
    "            Term frequency weighing:\n",
    "                * `b` - binary,\n",
    "                * `t` or `n` - raw,\n",
    "                * `a` - augmented,\n",
    "                * `l` - logarithm,\n",
    "                * `d` - double logarithm,\n",
    "                * `L` - log average.\n",
    "\n",
    "            Document frequency weighting:\n",
    "                * `x` or `n` - none,\n",
    "                * `f` - idf,\n",
    "                * `t` - zero-corrected idf,\n",
    "                * `p` - probabilistic idf.\n",
    "\n",
    "            Document normalization:\n",
    "                * `x` or `n` - none,\n",
    "                * `c` - cosine,\n",
    "                * `u` - pivoted unique,\n",
    "                * `b` - pivoted character length.\n",
    "\n",
    "            Default is 'nfc'.\n",
    "            For more information visit `SMART Information Retrieval System\n",
    "            <https://en.wikipedia.org/wiki/SMART_Information_Retrieval_System>`_.\n",
    "        pivot : float or None, optional\n",
    "            In information retrieval, TF-IDF is biased against long documents [1]_. Pivoted document length\n",
    "            normalization solves this problem by changing the norm of a document to `slope * old_norm + (1.0 -\n",
    "            slope) * pivot`.\n",
    "\n",
    "            You can either set the `pivot` by hand, or you can let Gensim figure it out automatically with the following\n",
    "            two steps:\n",
    "\n",
    "                * Set either the `u` or `b` document normalization in the `smartirs` parameter.\n",
    "                * Set either the `corpus` or `dictionary` parameter. The `pivot` will be automatically determined from\n",
    "                  the properties of the `corpus` or `dictionary`.\n",
    "\n",
    "            If `pivot` is None and you don't follow steps 1 and 2, then pivoted document length normalization will be\n",
    "            disabled. Default is None.\n",
    "\n",
    "            See also the blog post at https://rare-technologies.com/pivoted-document-length-normalisation/.\n",
    "        slope : float, optional\n",
    "            In information retrieval, TF-IDF is biased against long documents [1]_. Pivoted document length\n",
    "            normalization solves this problem by changing the norm of a document to `slope * old_norm + (1.0 -\n",
    "            slope) * pivot`.\n",
    "\n",
    "            Setting the `slope` to 0.0 uses only the `pivot` as the norm, and setting the `slope` to 1.0 effectively\n",
    "            disables pivoted document length normalization. Singhal [2]_ suggests setting the `slope` between 0.2 and\n",
    "            0.3 for best results. Default is 0.25.\n",
    "\n",
    "            See also the blog post at https://rare-technologies.com/pivoted-document-length-normalisation/.\n",
    "\n",
    "        References\n",
    "        ----------\n",
    "        .. [1] Singhal, A., Buckley, C., & Mitra, M. (1996). `Pivoted Document Length\n",
    "           Normalization <http://singhal.info/pivoted-dln.pdf>`_. *SIGIR Forum*, 51, 176–184.\n",
    "        .. [2] Singhal, A. (2001). `Modern information retrieval: A brief overview <http://singhal.info/ieee2001.pdf>`_.\n",
    "           *IEEE Data Eng. Bull.*, 24(4), 35–43.\n",
    "\n",
    "        \"\"\"\n",
    "        self.id2word = id2word\n",
    "        self.wlocal, self.wglobal, self.normalize = wlocal, wglobal, normalize\n",
    "        self.num_docs, self.num_nnz, self.idfs = None, None, None\n",
    "        self.smartirs = resolve_weights(smartirs) if smartirs is not None else None\n",
    "        self.slope = slope\n",
    "        self.pivot = pivot\n",
    "        self.eps = 1e-12\n",
    "\n",
    "        if smartirs is not None:\n",
    "            n_tf, n_df, n_n = self.smartirs\n",
    "            self.wlocal = partial(smartirs_wlocal, local_scheme=n_tf)\n",
    "            self.wglobal = partial(smartirs_wglobal, global_scheme=n_df)\n",
    "\n",
    "        if dictionary is not None:\n",
    "            # user supplied a Dictionary object, which already contains all the\n",
    "            # statistics we need to construct the IDF mapping. we can skip the\n",
    "            # step that goes through the corpus (= an optimization).\n",
    "            if corpus is not None:\n",
    "                logger.warning(\n",
    "                    \"constructor received both corpus and explicit inverse document frequencies; ignoring the corpus\"\n",
    "                )\n",
    "            self.num_docs, self.num_nnz = dictionary.num_docs, dictionary.num_nnz\n",
    "            self.cfs = dictionary.cfs.copy()\n",
    "            self.dfs = dictionary.dfs.copy()\n",
    "            self.term_lens = {termid: len(term) for termid, term in dictionary.items()}\n",
    "            self.idfs = precompute_idfs(self.wglobal, self.dfs, self.num_docs)\n",
    "            if id2word is None:\n",
    "                self.id2word = dictionary\n",
    "        elif corpus is not None:\n",
    "            self.initialize(corpus)\n",
    "        else:\n",
    "            # NOTE: everything is left uninitialized; presumably the model will\n",
    "            # be initialized in some other way\n",
    "            pass\n",
    "\n",
    "        # If smartirs is not None, override pivot and normalize\n",
    "        if smartirs is None:\n",
    "            return\n",
    "        if self.pivot is not None:\n",
    "            if n_n in 'ub':\n",
    "                logger.warning(\"constructor received pivot; ignoring smartirs[2]\")\n",
    "            return\n",
    "        if n_n in 'ub' and callable(self.normalize):\n",
    "            logger.warning(\"constructor received smartirs; ignoring normalize\")\n",
    "        if n_n in 'ub' and not dictionary and not corpus:\n",
    "            logger.warning(\"constructor received no corpus or dictionary; ignoring smartirs[2]\")\n",
    "        elif n_n == \"u\":\n",
    "            self.pivot = 1.0 * self.num_nnz / self.num_docs\n",
    "        elif n_n == \"b\":\n",
    "            self.pivot = 1.0 * sum(\n",
    "                self.cfs[termid] * (self.term_lens[termid] + 1.0) for termid in dictionary.keys()\n",
    "            ) / self.num_docs\n",
    "\n",
    "    @classmethod\n",
    "    def load(cls, *args, **kwargs):\n",
    "        \"\"\"Load a previously saved TfidfModel class. Handles backwards compatibility from\n",
    "        older TfidfModel versions which did not use pivoted document normalization.\n",
    "\n",
    "        \"\"\"\n",
    "        model = super(TfidfModel, cls).load(*args, **kwargs)\n",
    "        if not hasattr(model, 'pivot'):\n",
    "            model.pivot = None\n",
    "            logger.info('older version of %s loaded without pivot arg', cls.__name__)\n",
    "            logger.info('Setting pivot to %s.', model.pivot)\n",
    "        if not hasattr(model, 'slope'):\n",
    "            model.slope = 0.65\n",
    "            logger.info('older version of %s loaded without slope arg', cls.__name__)\n",
    "            logger.info('Setting slope to %s.', model.slope)\n",
    "        if not hasattr(model, 'smartirs'):\n",
    "            model.smartirs = None\n",
    "            logger.info('older version of %s loaded without smartirs arg', cls.__name__)\n",
    "            logger.info('Setting smartirs to %s.', model.smartirs)\n",
    "        return model\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"%s<num_docs=%s, num_nnz=%s>\" % (self.__class__.__name__, self.num_docs, self.num_nnz)\n",
    "\n",
    "    def initialize(self, corpus):\n",
    "        \"\"\"Compute inverse document weights, which will be used to modify term frequencies for documents.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        corpus : iterable of iterable of (int, int)\n",
    "            Input corpus.\n",
    "\n",
    "        \"\"\"\n",
    "        logger.info(\"collecting document frequencies\")\n",
    "        dfs = {}\n",
    "        numnnz, docno = 0, -1\n",
    "\n",
    "        for docno, bow in enumerate(corpus):\n",
    "            if docno % 10000 == 0:\n",
    "                logger.info(\"PROGRESS: processing document #%i\", docno)\n",
    "            numnnz += len(bow)\n",
    "            for termid, _ in bow:\n",
    "                dfs[termid] = dfs.get(termid, 0) + 1\n",
    "        # keep some stats about the training corpus\n",
    "        self.num_docs = docno + 1\n",
    "        self.num_nnz = numnnz\n",
    "        self.cfs = None\n",
    "        self.dfs = dfs\n",
    "        self.term_lengths = None\n",
    "        # and finally compute the idf weights\n",
    "        self.idfs = precompute_idfs(self.wglobal, self.dfs, self.num_docs)\n",
    "        self.add_lifecycle_event(\n",
    "            \"initialize\",\n",
    "            msg=(\n",
    "                f\"calculated IDF weights for {self.num_docs} documents and {max(dfs.keys()) + 1 if dfs else 0}\"\n",
    "                f\" features ({self.num_nnz} matrix non-zeros)\"\n",
    "            ),\n",
    "        )\n",
    "\n",
    "    def __getitem__(self, bow, eps=1e-12):\n",
    "        \"\"\"Get the tf-idf representation of an input vector and/or corpus.\n",
    "\n",
    "        bow : {list of (int, int), iterable of iterable of (int, int)}\n",
    "            Input document in the `sparse Gensim bag-of-words format\n",
    "            <https://radimrehurek.com/gensim/intro.html#core-concepts>`_,\n",
    "            or a streamed corpus of such documents.\n",
    "        eps : float\n",
    "            Threshold value, will remove all position that have tfidf-value less than `eps`.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        vector : list of (int, float)\n",
    "            TfIdf vector, if `bow` is a single document\n",
    "        :class:`~gensim.interfaces.TransformedCorpus`\n",
    "            TfIdf corpus, if `bow` is a corpus.\n",
    "\n",
    "        \"\"\"\n",
    "        self.eps = eps\n",
    "        # if the input vector is in fact a corpus, return a transformed corpus as a result\n",
    "        is_corpus, bow = utils.is_corpus(bow)\n",
    "        if is_corpus:\n",
    "            return self._apply(bow)\n",
    "\n",
    "        # unknown (new) terms will be given zero weight (NOT infinity/huge weight,\n",
    "        # as strict application of the IDF formula would dictate)\n",
    "\n",
    "        termid_array, tf_array = [], []\n",
    "        for termid, tf in bow:\n",
    "            termid_array.append(termid)\n",
    "            tf_array.append(tf)\n",
    "\n",
    "        tf_array = self.wlocal(np.array(tf_array))\n",
    "\n",
    "        vector = [\n",
    "            (termid, tf * self.idfs.get(termid))\n",
    "            for termid, tf in zip(termid_array, tf_array) if abs(self.idfs.get(termid, 0.0)) > self.eps\n",
    "        ]\n",
    "\n",
    "        # and finally, normalize the vector either to unit length, or use a\n",
    "        # user-defined normalization function\n",
    "        if self.smartirs:\n",
    "            n_n = self.smartirs[2]\n",
    "            if n_n == \"n\" or (n_n in 'ub' and self.pivot is None):\n",
    "                if self.pivot is not None:\n",
    "                    _, old_norm = matutils.unitvec(vector, return_norm=True)\n",
    "                norm_vector = vector\n",
    "            elif n_n == \"c\":\n",
    "                if self.pivot is not None:\n",
    "                    _, old_norm = matutils.unitvec(vector, return_norm=True)\n",
    "                else:\n",
    "                    norm_vector = matutils.unitvec(vector)\n",
    "            elif n_n == \"u\":\n",
    "                _, old_norm = matutils.unitvec(vector, return_norm=True, norm='unique')\n",
    "            elif n_n == \"b\":\n",
    "                old_norm = sum(freq * (self.term_lens[termid] + 1.0) for termid, freq in bow)\n",
    "        else:\n",
    "            if self.normalize is True:\n",
    "                self.normalize = matutils.unitvec\n",
    "            elif self.normalize is False:\n",
    "                self.normalize = utils.identity\n",
    "\n",
    "            if self.pivot is not None:\n",
    "                _, old_norm = self.normalize(vector, return_norm=True)\n",
    "            else:\n",
    "                norm_vector = self.normalize(vector)\n",
    "\n",
    "        if self.pivot is None:\n",
    "            norm_vector = [(termid, weight) for termid, weight in norm_vector if abs(weight) > self.eps]\n",
    "        else:\n",
    "            pivoted_norm = (1 - self.slope) * self.pivot + self.slope * old_norm\n",
    "            norm_vector = [\n",
    "                (termid, weight / float(pivoted_norm))\n",
    "                for termid, weight in vector\n",
    "                if abs(weight / float(pivoted_norm)) > self.eps\n",
    "            ]\n",
    "        return norm_vector\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
